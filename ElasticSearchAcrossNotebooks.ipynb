{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a59f0f6d-f845-4c59-8216-678a5c049dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch, RequestsHttpConnection\n",
    "import nbformat as nbf\n",
    "import warnings\n",
    "import requests\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4427dd41-ca83-4e50-be17-0c9913a040f0",
   "metadata": {},
   "source": [
    "The problem? I have a ton of Jupyter Notebooks and I need an easy to search through them all. I am always remembering that I have some snippet of code <i>somewhere</i> in these notebooks so need an easy to way to find it. \n",
    "\n",
    "Enter <b>Elasticsearch</b>\n",
    "\n",
    "Elasticsearch \"is a search engine based on the Lucene library. It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents\" That means that you can put lots of documents full of text into it, and it will index all of this, and make it easy to search\n",
    "\n",
    "The Elasticsearch folks have also built <b>Kibana</b>, which is \"proprietary data visualization dashboard software for Elasticsearch\". That means that Kibana is a handy GUI tool you can use to quickly search your data, similiar to using something like a Google searchbox\n",
    "\n",
    "In this notebook, I will demonstrate how to: \n",
    "\n",
    "1. Setup Docker containers for Elasticsearch and Kibana on a shared Docker network\n",
    "2. Use nbconvert to convert your .ipynb files to a list of strings\n",
    "3. Upload that list of strings into a Elasticsearch database\n",
    "4. Search through the notebooks using either the Python Elasticsearch library, or Kibana\n",
    "\n",
    "<i>Prerequisites:</i><br/> That you know how to create a Jupyter Notebook and save it somewhere, and that you have Docker installed. \n",
    "\n",
    "<i>Caveat:</i><br/>This assumes your use case is that you want to easily be able to search through your own notebooks as part of your workflow, and as such I am going to ignore some Elasticsearch security options. Which is why I have an ignore warnings filter in this notebook. But don't use this approach if you are planning somehing that is not dev. \n",
    "\n",
    "This will all take about 5 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a4633a-fe9b-4e77-b950-7a2ae772dcde",
   "metadata": {},
   "source": [
    "<b>Set up and Hello World</b>\n",
    "\n",
    "Let's start with some setup. Open a Terminal on your machine (bash on Linux, PowerShell on Windows, whatever). Then run the following commands. \n",
    "1. <code>docker network create elastic</code><br/>\n",
    "This wil tell Docker to create a docker network called 'elastic'\n",
    "2. <code>docker pull docker.elastic.co/elasticsearch/elasticsearch:7.13.1</code><br/>\n",
    "This will pull down an image of the latest version of elasticsearch (7.13.1) at the time of writing this\n",
    "3. <code>docker run --name es01-test --net elastic -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:7.13.1</code><br/>\n",
    "This will create a docker container on the docker network 'elastic', expose some ports so you can acess it. If you go to localhost:9200 you will see a welcom message\n",
    "\n",
    "So that's it for installing and getting Elasticsearch up and running. Now let's do the same for Kibana. Open a new shell and run the following commands:\n",
    "\n",
    "4. <code>docker pull docker.elastic.co/kibana/kibana:7.13.1</code><br/>\n",
    "This will pull down and image of Kibana\n",
    "5. <code>docker run --name kib01-test --net elastic -p 5601:5601 -e \"ELASTICSEARCH_HOSTS=http://es01-test:9200\" docker.elastic.co/kibana/kibana:7.13.1</code><br/>\n",
    "This will create a container for Kibana. It will see the Elasticsearch instance and be connected to it. You can go to localhost:5601 and see the Kibana homepage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00343f18-ad1f-4f09-8ebe-1c5eef56b18a",
   "metadata": {},
   "source": [
    "So looks like it is working. Let's make sure our notebook can see it to: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9db10091-0a76-4c06-b363-527292b7df6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\\n  \"name\" : \"829766e7847b\",\\n  \"cluster_name\" : \"docker-cluster\",\\n  \"cluster_uuid\" : \"mLWu9gbQQqqOy5xB3IONVg\",\\n  \"version\" : {\\n    \"number\" : \"7.13.1\",\\n    \"build_flavor\" : \"default\",\\n    \"build_type\" : \"docker\",\\n    \"build_hash\" : \"9a7758028e4ea59bcab41c12004603c5a7dd84a9\",\\n    \"build_date\" : \"2021-05-28T17:40:59.346932922Z\",\\n    \"build_snapshot\" : false,\\n    \"lucene_version\" : \"8.8.2\",\\n    \"minimum_wire_compatibility_version\" : \"6.8.0\",\\n    \"minimum_index_compatibility_version\" : \"6.0.0-beta1\"\\n  },\\n  \"tagline\" : \"You Know, for Search\"\\n}\\n'\n"
     ]
    }
   ],
   "source": [
    "res = requests.get('http://host.docker.internal:9200')\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2002763-d7ec-42ea-8a40-f25c01490de7",
   "metadata": {},
   "source": [
    "So this notebook can connect to the Elasticsearch instance. Note that I am using <code>host.docker.internal</code> in my URL in that get request. This is because I have set up my Jupyter up in Docker as well (details at: https://jupyter-docker-stacks.readthedocs.io/en/latest/using/selecting.html). If you have installed an Anaconda instance or something, this URL would be <code>localhost</code> rather than <code>host.docker.internal</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c91adaa-7b40-4147-9f15-f6e78d4ca000",
   "metadata": {},
   "source": [
    "Now, using our Python elasticsearch library, let's create a connection to Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6f57ee4-2c81-42d9-8ddb-d0c8bad5feaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note \"host.docker.internal\" might be \"localhost\" if you are running an Anaconda version of Jupyter\n",
    "es = Elasticsearch(hosts=[{\"host\": \"host.docker.internal\", \"port\": 9200}], \n",
    "                   connection_class=RequestsHttpConnection, max_retries=30,\n",
    "                       retry_on_timeout=True, request_timeout=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada092df-e64f-43f8-aa6b-3d24017d416a",
   "metadata": {},
   "source": [
    "Let's create an index (think of this as a db) and put some data into it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fb87b35-674a-4080-a698-8b5d5d83b3dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_index': 'testing-index',\n",
       " '_type': 'test',\n",
       " '_id': '1',\n",
       " '_version': 3,\n",
       " 'result': 'created',\n",
       " '_shards': {'total': 2, 'successful': 1, 'failed': 0},\n",
       " '_seq_no': 8,\n",
       " '_primary_term': 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#index some test data\n",
    "es.index(index='testing-index', doc_type='test', id=1, body={'test': 'test'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a461137a-5cb3-4ba9-9af4-2c6c72ba8b80",
   "metadata": {},
   "source": [
    "We can go to <code>http://localhost:9200/testing-index/_search?pretty=true&q=*:*</code> and see the data now exists in Elasticsearch. Or we could just retreive it using the Python elastic search library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96f595fb-80eb-4b80-970b-e71272c587d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_index': 'testing-index',\n",
       " '_type': '_doc',\n",
       " '_id': '1',\n",
       " '_version': 3,\n",
       " '_seq_no': 8,\n",
       " '_primary_term': 1,\n",
       " 'found': True,\n",
       " '_source': {'test': 'test'}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = es.get(index= \"testing-index\", id=1)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0a8ab9-245f-450c-b545-2a62f1bda4ff",
   "metadata": {},
   "source": [
    "So that works. Let's delete it now: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51085a70-07dc-4a4c-ace7-b883d9f9f8fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_index': 'testing-index',\n",
       " '_type': 'test',\n",
       " '_id': '1',\n",
       " '_version': 4,\n",
       " 'result': 'deleted',\n",
       " '_shards': {'total': 2, 'successful': 1, 'failed': 0},\n",
       " '_seq_no': 9,\n",
       " '_primary_term': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.delete(index='testing-index', doc_type='test', id=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f795a66-6fa9-4d1d-a3cc-49b6ca87d3aa",
   "metadata": {},
   "source": [
    "<b>Extracting Text from Jupyter Notebooks</b>\n",
    "\n",
    "Now let's do something a little more substantial. I have a folder full of Jupyter Notebooks and I always need code from one or another. So let's create a function to extract all the text from the notebooks. First I need a list of names of the Jupyter Notebooks from the directory in which they are located: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "657628bc-6a58-4e19-9cf4-2546fd8e1105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../work/HTMNotebooks/./HTMTest.ipynb',\n",
       " '../work/HTMNotebooks/./HTM_Encoders_0.ipynb',\n",
       " '../work/HTMNotebooks/./HTM_Overview_0.ipynb',\n",
       " '../work/HTMNotebooks/./HTM_Overview_1.ipynb',\n",
       " '../work/HTMNotebooks/./HTM_Overview_10.ipynb',\n",
       " '../work/HTMNotebooks/./HTM_Overview_11.ipynb',\n",
       " '../work/HTMNotebooks/./HTM_Overview_2.ipynb',\n",
       " '../work/HTMNotebooks/./HTM_Overview_3.ipynb',\n",
       " '../work/HTMNotebooks/./HTM_Overview_4.ipynb',\n",
       " '../work/HTMNotebooks/./HTM_Overview_5.ipynb',\n",
       " '../work/HTMNotebooks/./HTM_Overview_6.ipynb',\n",
       " '../work/HTMNotebooks/./HTM_Overview_7.ipynb',\n",
       " '../work/HTMNotebooks/./HTM_Overview_8.ipynb',\n",
       " '../work/HTMNotebooks/./HTM_Overview_9.ipynb']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathToLocationJupyterNotebookFiles = \"../work/HTMNotebooks/\"\n",
    "jupyterNotebooksFileNames = glob.glob(pathToLocationJupyterNotebookFiles + './*.ipynb')\n",
    "jupyterNotebooksFileNames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6988c7e0-02d2-40ac-aed2-54dd7e67eead",
   "metadata": {},
   "source": [
    "Now, let's create function to extract for the notebooks, and then we will just iterate each of the notebooks to extrat the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1abc9e0-373a-4893-ae16-8e750489fcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_VERSION = 4\n",
    "\n",
    "def extractTextFromNotebook(notebook_str):\n",
    "    formatted = nbf.read(notebook_str, as_version=NB_VERSION)\n",
    "    text = []\n",
    "    for cell in formatted.get('cells', []):\n",
    "        if 'source' in cell and 'cell_type' in cell:\n",
    "            if cell['cell_type'] == 'code' or cell['cell_type'] == 'markdown':\n",
    "                text.append(cell['source'])\n",
    "\n",
    "    return(text)\n",
    "\n",
    "\n",
    "textFromNotebooks = [extractTextFromNotebook(jupyterNotebooksFileNames[i]) for i in range(len(jupyterNotebooksFileNames))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa0bb3b-58a4-4d94-a3da-510ea38dc92c",
   "metadata": {},
   "source": [
    "We get back a list of lists of all the notebooks. Let's check we are on the right trck by looking at, say, the fith item in the first notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d3d37eb-88d6-4a1f-955d-31e4b90b0030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from htm.bindings.sdr import SDR, Metrics\\nfrom htm.encoders.rdse import RDSE, RDSE_Parameters\\nfrom htm.encoders.date import DateEncoder\\nfrom htm.bindings.algorithms import SpatialPooler\\nfrom htm.bindings.algorithms import TemporalMemory\\nfrom htm.algorithms.anomaly_likelihood import AnomalyLikelihood \\nfrom htm.bindings.algorithms import Predictor'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFromNotebooks[1][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95b0368-220c-43da-bc38-4f0167d89582",
   "metadata": {},
   "source": [
    "Now let's iterate through all those notebooks converted to text, and push them into Elasticsearch. Elasticsearch will want something JSON like so that's is what we will give it. I notice it seems to return an empty object but still appears to work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be1fbbb7-1cb5-43b9-9068-92f9af13ba32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elasticDBName = \"j-notebook-cell-search-index\"\n",
    "\n",
    "def writeTextCellsToElasticSearchDB(doc, notebookFilePath):\n",
    "    for i in range(len(doc)):\n",
    "        cellDict = {}\n",
    "        cellDict['text'] =  doc[i],\n",
    "        cellDict['noteBookFilePath'] =  notebookFilePath\n",
    "        es.index(index= elasticDBName, doc_type= 'cell', body=cellDict)\n",
    "    \n",
    "[writeTextCellsToElasticSearchDB(textFromNotebooks[i], jupyterNotebooksFileNames[i]) for i in range(len(jupyterNotebooksFileNames))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a52969-be97-4857-a360-ca33141e87e0",
   "metadata": {},
   "source": [
    "<b>Searching Notebooks</b>\n",
    "\n",
    "So now all the data is in Elasticsearch. Now we want to search it. There are three options to do this: \n",
    "\n",
    "1. You can use the Python elasticsearch library to run queries<br/>\n",
    "This can be quite handy I will cover some examples below\n",
    "2. Use Kibana to search<br/>\n",
    "This if fun to use, but I probably won't use it enough remember a Kibana proprietry query langage, (I can barely remember SQL these days). But this does allow a GUI search box and filters and all that. \n",
    "3. I could pass query params in a url to search, such as <code>http://localhost:9200/testing-index/_search?pretty=true&q=*:*</code><br/>\n",
    "If you are into this kind of thing, like if you love Postman or something it could be handy I guess. For our purposes I wouldn't do this, and won't cover it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d07a6bc-4127-426b-8036-c4cfa63cae0c",
   "metadata": {},
   "source": [
    "<b>Option 1: Using Python</b>\n",
    "\n",
    "This is my preferred way of doing it. Here are some handy getting started searches you can do to look through your data that has been put into Elasticsearch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d55ae1ef-7f0d-4b37-9a63-ebb3b9c1bf01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_index': 'j-notebook-cell-search-index',\n",
       " '_type': '_doc',\n",
       " '_id': 'VFtpKHoB3T1ThL6Sx1Yg',\n",
       " '_version': 1,\n",
       " '_seq_no': 0,\n",
       " '_primary_term': 1,\n",
       " 'found': True,\n",
       " '_source': {'text': ['import csv\\nimport datetime\\nimport os\\nimport numpy as np\\nimport random\\nimport math\\n\\nfrom htm.bindings.sdr import SDR, Metrics\\nfrom htm.encoders.rdse import RDSE, RDSE_Parameters\\nfrom htm.encoders.date import DateEncoder\\nfrom htm.bindings.algorithms import SpatialPooler\\nfrom htm.bindings.algorithms import TemporalMemory\\nfrom htm.algorithms.anomaly_likelihood import AnomalyLikelihood #FIXME use TM.anomaly instead, but it gives worse results than the py.AnomalyLikelihood now\\nfrom htm.bindings.algorithms import Predictor'],\n",
       "  'noteBookFilePath': '../work/HTMNotebooks/./HTMTest.ipynb'}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grab a particular record - note I just got the ID from http://localhost:9200/j-notebook-cell-search-index/_search?pretty=true&q=*:*\n",
    "# now the elastic search index is up and running\n",
    "es.get(index=elasticDBName, \n",
    "       doc_type=\"_doc\", id = \"VFtpKHoB3T1ThL6Sx1Yg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846c61bb-e0e2-4638-938d-836f7041c902",
   "metadata": {},
   "source": [
    "It supports all kind of queries to match text, partial match, etc. Here is another example, the use case here is that I know I have a notebook where I have done some work on Baltimore Crime Data, but can't remember where. So will put in the prefix \"crim\" and let Elasticsearch do its thing. \n",
    "\n",
    "Note that things can get a bit messy, so I would advise you to keep you query in a seperate Python dictionary, and then just pass that into the search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca0b54a5-58a0-493f-985d-93b2569e51c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'took': 1,\n",
       " 'timed_out': False,\n",
       " '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0},\n",
       " 'hits': {'total': {'value': 3, 'relation': 'eq'},\n",
       "  'max_score': 1.0,\n",
       "  'hits': [{'_index': 'j-notebook-cell-search-index',\n",
       "    '_type': 'cell',\n",
       "    '_id': 'OltpKHoB3T1ThL6Szlec',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'text': ['To explore this, let\\'s use some data. There is some really interesting data that will turn up in episode\\'s 7 and 8 in the context of the Spatial Pooler, that has some interesting info, but for now, let\\'s use Baltimore Crime Data. This data has nice coverage across a number of data points, descriptive names, some categorical variables, the footprint isn\\'t too big but it gives us a nice sample of 96k records\\n\\nInformation available <a href=\"https://data.baltimorecity.gov/datasets/baltimore::part1-crime-2015-to-2016/about\">https://data.baltimorecity.gov/datasets/baltimore::part1-crime-2015-to-2016/about</a>\\n'],\n",
       "     'noteBookFilePath': '../work/HTMNotebooks/./HTM_Overview_5.ipynb'}},\n",
       "   {'_index': 'j-notebook-cell-search-index',\n",
       "    '_type': 'cell',\n",
       "    '_id': 'PVtpKHoB3T1ThL6Szlez',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'text': ['df = pd.read_csv(\"./data/Part1_Crime_2015_to__2016.csv\")\\ndf.CrimeDateTime = df.CrimeDateTime.str.slice(0, -8)\\ndf.CrimeDateTime= pd.to_datetime(df.CrimeDateTime)\\ndf[\\'weekdayCodeWhenEventReported\\'] = [d.weekday() for d in df.CrimeDateTime]\\ndf[\\'monthCodeWhenEventReported\\'] = df[\\'CrimeDateTime\\'].dt.month\\ndf[\\'seasonCodeWhenEventReported\\'] = (df[\\'CrimeDateTime\\'].dt.month - 1) % 4\\ndf[\\'isWeekend\\'] = np.where(df.weekdayCodeWhenEventReported > 4, True, False)\\ndf = df.drop(\\'VRIName\\', axis=1)\\ndf = df.drop(\\'HashedRecord\\', axis=1)\\ndf = df.drop(\\'ObjectId\\', axis=1)'],\n",
       "     'noteBookFilePath': '../work/HTMNotebooks/./HTM_Overview_5.ipynb'}},\n",
       "   {'_index': 'j-notebook-cell-search-index',\n",
       "    '_type': 'cell',\n",
       "    '_id': '0ltpKHoB3T1ThL6S01cp',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'text': [\"<h2>HTM Overview 9: Boosting</h2>\\n\\nSo now let's start working with. The first thing we want to do is create a Scalar Encoder\\n\\nCrime data\"],\n",
       "     'noteBookFilePath': '../work/HTMNotebooks/./HTM_Overview_9.ipynb'}}]}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = {\n",
    "  \"query\": {\n",
    "      \"prefix\": {\n",
    "          \"text\": \"crim\"\n",
    "      }\n",
    "  }}\n",
    "\n",
    "es.search(index=elasticDBName, \n",
    "          body = q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e3b8c6-1e0a-43a3-8ae9-fbc8a132b886",
   "metadata": {},
   "source": [
    "<b>Option 2: Using Kibana</b>\n",
    "\n",
    "Kibana's cool, but after a while I did get annoyed at the UI But if you are going to use it\n",
    "\n",
    "1. Go to <code>http://localhost:5601/app/home#/</code> which should be up and running\n",
    "2. From the dropdown on the left, go the \"Stack Management\" menu item. This will take you to <code>http://localhost:5601/app/management</code>\n",
    "3. Choose the index pattern option. You will be taken to <code>http://localhost:5601/app/management/kibana/indexPatterns</code>\n",
    "4. Go to <code>http://localhost:5601/app/management/kibana/indexPatterns/create</code>\n",
    "5. Choose your index/database tha tis listed, and follow the prompts to set it up\n",
    "6. Then go back to <code>http://localhost:5601/app/home#/</code> and choose \"Discover\" from the left hand index\n",
    "\n",
    "From there you will have a search box and some filters, and all kinds of cool things you can check. \n",
    "\n",
    "Enjoy!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
