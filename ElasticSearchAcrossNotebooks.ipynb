{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a59f0f6d-f845-4c59-8216-678a5c049dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from elasticsearch import Elasticsearch, RequestsHttpConnection\n",
    "import nbformat as nbf\n",
    "import json\n",
    "import warnings\n",
    "import requests\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4427dd41-ca83-4e50-be17-0c9913a040f0",
   "metadata": {},
   "source": [
    "The problem? I have many Jupyter notebooks and I need an easy to search through them all. I am always remembering that I have some snippet of code <i>somewhere</i> in these notebooks so need an easy to way to find it. \n",
    "\n",
    "Enter <b>Elasticsearch</b>\n",
    "\n",
    "Elasticsearch \"is a search engine based on the Lucene library. It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents\" That means that you can put lots of documents full of text into it, and it will index all of this, and make it easy to search\n",
    "\n",
    "The Elasticsearch folks have also built <b>Kibana</b>, which is \"proprietary data visualization dashboard software for Elasticsearch\". That means that Kibana is a handy GUI tool you can use to quickly search your data, similiar to using something like a Google searchbox\n",
    "\n",
    "In this notebook, I will demonstrate how to: \n",
    "\n",
    "1. Create Docker containers for Elasticsearch and Kibana on a shared Docker network\n",
    "2. Use nbconvert to convert your .ipynb files to a list of strings\n",
    "3. Upload that list of strings into a Elasticsearch database\n",
    "4. Search through the notebooks using either the Python Elasticsearch library, or Kibana\n",
    "\n",
    "<i>Prerequisites:</i><br/> That you know how to create a Jupyter Notebook and save it somewhere, and that you have Docker installed. \n",
    "\n",
    "<i>Caveat:</i><br/>This assumes you want to easily just search through your own notebooks as part of your workflow, and as such I am going to ignore some Elasticsearch security options. Which is why I am ignoring warnings in this notebook. So don't use this approach if you are planning somehing that is not dev. \n",
    "\n",
    "This will all take about 5 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a4633a-fe9b-4e77-b950-7a2ae772dcde",
   "metadata": {},
   "source": [
    "<b>Set up and Hello World</b>\n",
    "\n",
    "Let's start with some setup. Open a Terminal on your machine (bash on Linux, PowerShell on Windows, whatever). Then run the following commands. \n",
    "1. <code>docker network create elastic</code><br/>\n",
    "This wil tell Docker to create a docker network called 'elastic'\n",
    "2. <code>docker pull docker.elastic.co/elasticsearch/elasticsearch:7.13.1</code><br/>\n",
    "This will pull down an image of the latest version of elasticsearch (7.13.1) at the time of writing this\n",
    "3. <code>docker run --name es01-test --net elastic -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:7.13.1</code><br/>\n",
    "This will create a docker container on the docker network 'elastic', expose some ports so you can acess it. If you go to localhost:9200 you will see a welcom message\n",
    "\n",
    "So that's it for installing and getting Elasticsearch up and running. Now let's do the same for Kibana. Open a new shell and run the following commands:\n",
    "\n",
    "4. <code>docker pull docker.elastic.co/kibana/kibana:7.13.1</code><br/>\n",
    "This will pull down and image of Kibana\n",
    "5. <code>docker run --name kib01-test --net elastic -p 5601:5601 -e \"ELASTICSEARCH_HOSTS=http://es01-test:9200\" docker.elastic.co/kibana/kibana:7.13.1</code><br/>\n",
    "This will create a container for Kibana. It will see the Elasticsearch instance and be connected to it. You can go to localhost:5601 and see the Kibana homepage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00343f18-ad1f-4f09-8ebe-1c5eef56b18a",
   "metadata": {},
   "source": [
    "So looks like it is working. Let's make sure our notebook can see it to: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9db10091-0a76-4c06-b363-527292b7df6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\\n  \"name\" : \"829766e7847b\",\\n  \"cluster_name\" : \"docker-cluster\",\\n  \"cluster_uuid\" : \"mLWu9gbQQqqOy5xB3IONVg\",\\n  \"version\" : {\\n    \"number\" : \"7.13.1\",\\n    \"build_flavor\" : \"default\",\\n    \"build_type\" : \"docker\",\\n    \"build_hash\" : \"9a7758028e4ea59bcab41c12004603c5a7dd84a9\",\\n    \"build_date\" : \"2021-05-28T17:40:59.346932922Z\",\\n    \"build_snapshot\" : false,\\n    \"lucene_version\" : \"8.8.2\",\\n    \"minimum_wire_compatibility_version\" : \"6.8.0\",\\n    \"minimum_index_compatibility_version\" : \"6.0.0-beta1\"\\n  },\\n  \"tagline\" : \"You Know, for Search\"\\n}\\n'\n"
     ]
    }
   ],
   "source": [
    "res = requests.get('http://host.docker.internal:9200')\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2002763-d7ec-42ea-8a40-f25c01490de7",
   "metadata": {},
   "source": [
    "So this notebook can connect to the Elasticsearch instance. Note that I am using <code>host.docker.internal</code> in my URL in that get request. This is because I have set up my Jupyter up in Docker as well (details at: https://jupyter-docker-stacks.readthedocs.io/en/latest/using/selecting.html). If you have installed an Anaconda instance or something, this URL would be <code>localhost</code> rather than <code>host.docker.internal</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c91adaa-7b40-4147-9f15-f6e78d4ca000",
   "metadata": {},
   "source": [
    "Now, using our Python elasticsearch library, let's create a connection to Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6f57ee4-2c81-42d9-8ddb-d0c8bad5feaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note \"host.docker.internal\" might be \"localhost\" if you are running an Anaconda version of Jupyter\n",
    "es = Elasticsearch(hosts=[{\"host\": \"host.docker.internal\", \"port\": 9200}], \n",
    "                   connection_class=RequestsHttpConnection, max_retries=30,\n",
    "                       retry_on_timeout=True, request_timeout=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada092df-e64f-43f8-aa6b-3d24017d416a",
   "metadata": {},
   "source": [
    "Let's create an index (think of this as a db) and put some data into it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fb87b35-674a-4080-a698-8b5d5d83b3dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_index': 'testing-index',\n",
       " '_type': 'test',\n",
       " '_id': '1',\n",
       " '_version': 1,\n",
       " 'result': 'created',\n",
       " '_shards': {'total': 2, 'successful': 1, 'failed': 0},\n",
       " '_seq_no': 2,\n",
       " '_primary_term': 1}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#index some test data\n",
    "es.index(index='testing-index', doc_type='test', id=1, body={'test': 'test'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a461137a-5cb3-4ba9-9af4-2c6c72ba8b80",
   "metadata": {},
   "source": [
    "We can go to <code>http://localhost:9200/testing-index/_search?pretty=true&q=*:*</code> and see the data now exists in Elasticsearch. Or we could just retreive it using the Python elastic search library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96f595fb-80eb-4b80-970b-e71272c587d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_index': 'testing-index',\n",
       " '_type': '_doc',\n",
       " '_id': '1',\n",
       " '_version': 1,\n",
       " '_seq_no': 2,\n",
       " '_primary_term': 1,\n",
       " 'found': True,\n",
       " '_source': {'test': 'test'}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = es.get(index= \"testing-index\", id=1)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0a8ab9-245f-450c-b545-2a62f1bda4ff",
   "metadata": {},
   "source": [
    "So that works. Let's delete it now: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51085a70-07dc-4a4c-ace7-b883d9f9f8fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_index': 'testing-index',\n",
       " '_type': 'test',\n",
       " '_id': '1',\n",
       " '_version': 2,\n",
       " 'result': 'deleted',\n",
       " '_shards': {'total': 2, 'successful': 1, 'failed': 0},\n",
       " '_seq_no': 3,\n",
       " '_primary_term': 1}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.delete(index='testing-index', doc_type='test', id=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f795a66-6fa9-4d1d-a3cc-49b6ca87d3aa",
   "metadata": {},
   "source": [
    "<b>Extracting Text from Jupyter Notebooks</b>\n",
    "\n",
    "Now let's do something a little more substantial. I have a folder full of Jupyter Notebooks and I always need code from one or another. So let's create a function to extract all the text from the notebooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1abc9e0-373a-4893-ae16-8e750489fcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_VERSION = 4\n",
    "\n",
    "def extractTextFromNotebook(notebook_str):\n",
    "    formatted = nbf.read(notebook_str, as_version=NB_VERSION)\n",
    "    text = []\n",
    "    for cell in formatted.get('cells', []):\n",
    "        if 'source' in cell and 'cell_type' in cell:\n",
    "            if cell['cell_type'] == 'code' or cell['cell_type'] == 'markdown':\n",
    "                text.append(cell['source'])\n",
    "\n",
    "    return(text)\n",
    "\n",
    "listOfStringsInNotebook = extractTextFromNotebook(\"../work/HTMNotebooks/HTM_Overview_7.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa0bb3b-58a4-4d94-a3da-510ea38dc92c",
   "metadata": {},
   "source": [
    "Let's check one of those list's of strings. It is definitely some text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d3d37eb-88d6-4a1f-955d-31e4b90b0030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'df = pd.read_csv(\"./data/gymdata.csv\", header=1)\\ndf = df.rename(columns={\"datetime\": \"date_time\", \"float\": \"power_consumption\"})\\ndf = df.iloc[1:]\\ndf.head()'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listOfStringsInNotebook[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95b0368-220c-43da-bc38-4f0167d89582",
   "metadata": {},
   "source": [
    "Now let's iterate through all those notebooks converted to text, and push them into Elasticsearch. Elasticsearch will want something JSON like so that's is what we will give it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be1fbbb7-1cb5-43b9-9068-92f9af13ba32",
   "metadata": {},
   "outputs": [],
   "source": [
    "elasticDBName = \"notebook-cell-search\"\n",
    "\n",
    "def writeTextCellsToElasticSearchDB(doc):\n",
    "    for i in range(len(doc)):\n",
    "        cellDict = {}\n",
    "        cellDict['text'] =  doc[i],\n",
    "        cellDict['timestamp'] =  datetime.now()\n",
    "        listOfStringsInNotebook.append(cellDict)\n",
    "        es.index(index= elasticDBName, doc_type= 'cell', body=cellDict, id = i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55fd06a3-36b2-4ca7-8262-568e1776a26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "writeTextCellsToElasticSearchDB(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a52969-be97-4857-a360-ca33141e87e0",
   "metadata": {},
   "source": [
    "<b>Searching Notebooks</b>\n",
    "\n",
    "So now all the data is in Elasticsearch. Now we want to search it. There are three options to do this: \n",
    "\n",
    "1. You can use the Python elasticsearch library to run queries<br/>\n",
    "This can be quite handy I will cover some examples below\n",
    "2. Use Kibana to search\n",
    "I tend to use this. I will never remember a proprietry query langage, I can barely remember SQL these days. So I want to just be able to quickly search in a search box where I am given some help to do so. Kibana is perfect for that:\n",
    "3. I could pass query params in a url to search, such as <code>http://localhost:9200/testing-index/_search?pretty=true&q=*:*</code><br/>\n",
    "If you are into this kind of thing, and love Postman or something it could be handy I guess. For our purposes I wouldn't do this, and won't cover it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d07a6bc-4127-426b-8036-c4cfa63cae0c",
   "metadata": {},
   "source": [
    "<b>Option 1: Using Python</b>\n",
    "\n",
    "This is my preferred way of doing it. Here are some handy getting started searches you can do to look through your data that has been put into Elasticsearch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d55ae1ef-7f0d-4b37-9a63-ebb3b9c1bf01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_index': 'notebook-cell-search',\n",
       " '_type': '_doc',\n",
       " '_id': '44',\n",
       " '_version': 2,\n",
       " '_seq_no': 91,\n",
       " '_primary_term': 1,\n",
       " 'found': True,\n",
       " '_source': {'text': ['def getColumHistorySummary(columsHistory, columnChoice, inputCount):\\n    activeColumnHistory = [columsHistory[i][columnChoice][\\'activeColumns\\'] for i in range(0, inputCount)]\\n    history = []\\n    for j in range(0, len(activeColumnHistory) - 1):\\n        columnHistoryConnectionsAndDisconnections = {}\\n\\n        currentColumnSynapses, nextColumnSynapses, = set(activeColumnHistory[j]), set(activeColumnHistory[j + 1])\\n        columnHistoryConnectionsAndDisconnections[\"newlyDisconnectedSynapses\"] = list(currentColumnSynapses - nextColumnSynapses)\\n        columnHistoryConnectionsAndDisconnections[\"newlyConnectedSynapses\"] = list(nextColumnSynapses - currentColumnSynapses)\\n        unchangedSynapses = np.sort(list(nextColumnSynapses - set(columnHistoryConnectionsAndDisconnections[\"newlyConnectedSynapses\"])))\\n\\n        columnHistoryConnectionsAndDisconnections[\"unchangedSynapses\"] = np.sort(list(nextColumnSynapses - set(columnHistoryConnectionsAndDisconnections[\"newlyConnectedSynapses\"])))\\n        columnHistoryConnectionsAndDisconnections[\"timeStep\"] = j + 1\\n        history.append(columnHistoryConnectionsAndDisconnections)\\n\\n    return(history)'],\n",
       "  'timestamp': '2021-06-14T05:34:56.789465'}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grab a particular record\n",
    "es.get(index='notebook-cell-search', \n",
    "       doc_type=\"_doc\", id = 44)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846c61bb-e0e2-4638-938d-836f7041c902",
   "metadata": {},
   "source": [
    "It supports all kind of queries to match text, partial match, etc. Here is another example. Things can get a bit messy, so I would advise you to keep you query in a seperate Python dict, and then just pass the Python dict to the search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca0b54a5-58a0-493f-985d-93b2569e51c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'took': 0,\n",
       " 'timed_out': False,\n",
       " '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0},\n",
       " 'hits': {'total': {'value': 6, 'relation': 'eq'},\n",
       "  'max_score': 1.0,\n",
       "  'hits': [{'_index': 'notebook-cell-search',\n",
       "    '_type': 'cell',\n",
       "    '_id': '3',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'text': ['ep7 = \\'<iframe style=\"background:#99ddff; color:black; padding: 10px\" width=\"400\" height=\"315\" src=\"https://www.youtube.com/embed/R5UoFNtv5AU\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\\'\\nep8 = \\'<iframe  style=\"background:#99ddff; color:black; padding: 10px\" width=\"400\" height=\"315\" src=\"https://www.youtube.com/embed/rHvjykCIrZM\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\\'\\ntable = \\'<table style=\"width:100%\"><tr><td>\\' + ep7 + \\'</td><td>\\' + ep8 + \\'</td></tr>\\'\\nHTML(table)'],\n",
       "     'timestamp': '2021-06-14T05:34:56.427812'}},\n",
       "   {'_index': 'notebook-cell-search',\n",
       "    '_type': 'cell',\n",
       "    '_id': '8',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'text': [\"Next, let's do bit of work on the data set, just to clean up the columns, to seperate date and time, and create an indicator of whether its weekend or not, so the data can easily be placed in a multi-encoder\"],\n",
       "     'timestamp': '2021-06-14T05:34:56.477095'}},\n",
       "   {'_index': 'notebook-cell-search',\n",
       "    '_type': 'cell',\n",
       "    '_id': '13',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'text': [\"With that out of the way, let's create an instance of a MultiEncoder and put our data into it. We will need four encoders, one for power consumption, one for date, one for time of day, and one for weekend:\"],\n",
       "     'timestamp': '2021-06-14T05:34:56.518159'}},\n",
       "   {'_index': 'notebook-cell-search',\n",
       "    '_type': 'cell',\n",
       "    '_id': '15',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'text': [\"Now let's put all those into a single encoder, the inputs ready for our spatial pooler. \"],\n",
       "     'timestamp': '2021-06-14T05:34:56.536258'}},\n",
       "   {'_index': 'notebook-cell-search',\n",
       "    '_type': 'cell',\n",
       "    '_id': '29',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'text': ['Next step, place our gym power consumption data in an encoder: '],\n",
       "     'timestamp': '2021-06-14T05:34:56.656789'}},\n",
       "   {'_index': 'notebook-cell-search',\n",
       "    '_type': 'cell',\n",
       "    '_id': '32',\n",
       "    '_score': 1.0,\n",
       "    '_source': {'text': ['# create enclder'],\n",
       "     'timestamp': '2021-06-14T05:34:56.685457'}}]}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = {\n",
    "  \"query\": {\n",
    "      \"prefix\": {\n",
    "          \"text\": \"enc\"\n",
    "      }\n",
    "  }}\n",
    "\n",
    "es.search(index=\"notebook-cell-search\", \n",
    "          body = q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d1aaeb-4bdb-429b-81fb-3418c73df413",
   "metadata": {},
   "source": [
    "I tend to avoid  widgets in noteobooks simply because I like to make them super easy to deploy, however this is an instnance where I might just set up. They way I like to set it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e07a185-64a6-4ca0-9b95-8f2fa83efc62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932ec93d-fc71-45e1-8072-31251bd6cde6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2e3b8c6-1e0a-43a3-8ae9-fbc8a132b886",
   "metadata": {},
   "source": [
    "<b>Option 2: Using Kibana</b>\n",
    "\n",
    "Kibana's cool, but after a while I did get annoyed at the UI - its kind of invasive. But if you are going to use it\n",
    "\n",
    "1. Go to <code>http://localhost:5601/app/home#/</code> which should be up and running\n",
    "2. From the dropdown on the left, go the \"Stack Management\" menu item. This will take you to <code>http://localhost:5601/app/management</code>\n",
    "3. Choose the index pattern option. You will be taken to <code>http://localhost:5601/app/management/kibana/indexPatterns</code>\n",
    "4. Go to <code>http://localhost:5601/app/management/kibana/indexPatterns/create</code>\n",
    "5. Choose notebook-cell-search, follow the prompts to set this index\n",
    "6. Then go back to <code>http://localhost:5601/app/home#/</code> and choose \"Discover\" from the left hand index\n",
    "\n",
    "From there you will have a search box and some filters, and all kinds of cool things you can check. Have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e15db7-6340-4db2-8c9b-9250da5dde8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
