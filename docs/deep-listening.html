<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Deep Listening Project - Jamie Gabriel</title>
    <style>
        body {
            font-family: sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 2rem;
            line-height: 1.6;
            color: #222;
            background-color: #fff;
        }

        h1 {
            text-align: left;
        }

        /* Flex layout for image on right */
        .intro-flex {
            display: flex;
            align-items: flex-start;
            gap: 2rem;
            margin: 2rem 0;
        }

        .intro-text {
            flex: 1;
        }

        .intro-image img {
            width: 150px;
            height: auto;
            display: block;
        }

        figure {
            margin: 1rem 0; /* reduced spacing globally */
        }

        figure:first-of-type {
            margin-top: 0.5rem;
        }

        figcaption h3 {
            margin: 0.2rem 0;
            font-size: 1.2rem;
        }

        figcaption h3 a {
            color: #000;
            text-decoration: none;
        }

        figcaption h3 .small {
            font-size: 0.9rem;
            color: #666;
        }

        a {
            color: #007bff;
            text-decoration: none;
        }

        a:hover {
            text-decoration: underline;
        }

        .toc ul {
            list-style-type: disc;
            margin-left: 2rem;
        }
    </style>
</head>
<body>
    <h1>Deep Listening</h1>
    <h2>Real-Time MIDI Pattern Analysis for Assisted Improvisation</h2>
    <a class="back-link" href="./index.html">← Back</a>

    <!-- Table of Contents -->
    <div class="toc">
        <h3>Contents</h3>
        <ul>
            <li><a href="#description">Overview</a></li>
                <li><a href="#why-different">How is this different</a></li>
            <li><a href="#background">Background Reading</a></li>
            <li><a href="#data-collection">Setting up for data dollection</a></li>
                    <li><a href="#related-code">Related code</a></li>
            <li><a href="#links">Links & References</a></li>
        </ul>
    </div>

    <div class="intro-flex" id="description">


        <div class="intro-text">
             <h3>Overview</h3>
           

          <p>The core idea of this project is to capture and analyse MIDI data in real time — whether from instruments, effects units, or controllers — to uncover the underlying structures of musical interaction. Each performance gesture, such as modulation sweeps on a Chase Bliss pedal or pitch manipulation on the Korg Modwave, is recorded, timestamped, and analysed live. These micro-decisions form a continuous data stream. This can then be visualised or transformed into responsive feedback to the performer, instrument, and evolving underlying deep learingmodel. I am aiming to capture all available MIDI information, including note, rhythmic, and synthesis parameters, as well as pedal and controller data, anything that can communicate via the MIDI protocol. Deep learning models are then applied to interpret these streams and generate new MIDI data in response, enriching and enhancing the live performance.</p>


            <p>This project is not about automating performance or mimicking the human ability to improvise and create in real-time. Rather, the idea here is to augment the improvisation experience it, find ways to make it more integrated into what is unfolding in live settings. When working in ensembles,  improvising musicians are in a constant process of perceiving their own and others’ musical decisions. This project is about finding ways to make that experience deeper and more immersive.</p>

            <p>The project brings together alot of the things that I am really interested in - improvisation, real-tme sound design, the use of advanced mathematics to augment the experience of collaboration, finding latent patterns in data. A lot of geeky stuff, sure, but the most important aspect of the project is to find ways to hear and experience music. It is not simply about pattern recognition for the sake finding patterns, it about creating ways to have richer musical interations.  </p>
            <p>
            The end goal is to find new ways to think about interactive creative performance, reframes MIDI not as a recording protocol, but as a language of interaction. It enables a new kind of deep listening where performers can explore emergent patterns, rhythmic densities, and collaborative dynamics as they happen — turning data into an active participant in the creative process. where data itself becomes a dynamic actor  dialogue between humans and the technolgy, leading to new creative experiences and outcomes.</p>
        </div>
    </div>

    <div class="why-different" id="why-different">
    	 <h3>How is this different?</h3>

  <p>There are a number of groups especially in the Music Information Retrieval (MIR) space, working with MIDI data. Most of them explore how MIDI can be used in post-hoc or offline modes. Traditional MIDI data studies focus on pattern mining, expressive timing, or style modeling after performance data has been captured. By contrast, this system listens and responds as the performance unfolds.This project this project is designed for real-time interpretation and feedback. </p>

  <p>The aim is not just to analyze musical data, but to augment creative awareness during performance. Every gesture — from a footswitch on the MC8 Pro to 



  </p>

  <section id="ismir-midi-research">
  <h2>Key ISMIR Researchers and Projects on MIDI Datasets</h2>
  <p>This table summarises leading contributions from the ISMIR community in developing and analysing MIDI datasets for music information retrieval (MIR), machine learning, and computational creativity research.</p>

  <table style="width:100%; border-collapse:collapse; margin-top:1em;">
    <thead>
      <tr style="background-color:#f0f0f0; text-align:left;">
        <th style="padding:10px; border-bottom:2px solid #ccc;">Researcher / Group</th>
        <th style="padding:10px; border-bottom:2px solid #ccc;">Project / Dataset</th>
        <th style="padding:10px; border-bottom:2px solid #ccc;">Focus / Contribution</th>
        <th style="padding:10px; border-bottom:2px solid #ccc;">Reference (APA7)</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td style="padding:10px;">Hawthorne, C. (Google Magenta)</td>
        <td style="padding:10px;">MAESTRO Dataset</td>
        <td style="padding:10px;">High-quality aligned audio–MIDI data from piano performances for transcription and generation tasks.</td>
        <td style="padding:10px;">Hawthorne, C. et al. (2018). *Enabling factorized piano music modeling and generation with the MAESTRO dataset*. ISMIR.</td>
      </tr>
      <tr>
        <td style="padding:10px;">Simon, I., Roberts, A. (Google Magenta)</td>
        <td style="padding:10px;">PerformanceRNN / Magenta MIDI Dataset</td>
        <td style="padding:10px;">Expressive performance modeling and real-time sequence generation using recurrent neural networks.</td>
        <td style="padding:10px;">Oore, S. et al. (2018). *This time with feeling: Learning expressive musical performance*. Neural Computing and Applications.</td>
      </tr>
      <tr>
        <td style="padding:10px;">Donahue, C. (UCSD)</td>
        <td style="padding:10px;">MIDI-DDSP</td>
        <td style="padding:10px;">Neural synthesis conditioned on MIDI control parameters for expressive sound generation.</td>
        <td style="padding:10px;">Huang, R., & Donahue, C. (2021). *MIDI-DDSP: Detailed control of musical performance via hierarchical modeling*. ISMIR.</td>
      </tr>
      <tr>
        <td style="padding:10px;">Raffel, C. (Columbia / Google)</td>
        <td style="padding:10px;">Lakh MIDI Dataset</td>
        <td style="padding:10px;">Large-scale collection of MIDI files aligned with Million Song Dataset for symbolic–audio mapping.</td>
        <td style="padding:10px;">Raffel, C. (2016). *Learning-based methods for comparing sequences, with applications to audio-to-MIDI alignment and matching*. PhD thesis, Columbia University.</td>
      </tr>
      <tr>
        <td style="padding:10px;">Kim, J., & Bello, J. P. (NYU)</td>
        <td style="padding:10px;">URMP Dataset</td>
        <td style="padding:10px;">Synchronized audio–video–MIDI dataset of chamber ensembles for multi-modal music analysis.</td>
        <td style="padding:10px;">Li, B., Kim, J., & Bello, J. P. (2018). *URMP: A dataset for multimodal music performance analysis*. ISMIR.</td>
      </tr>
      <tr>
        <td style="padding:10px;">MIDI Toolbox (Eerola & Toiviainen)</td>
        <td style="padding:10px;">MATLAB MIDI Toolbox</td>
        <td style="padding:10px;">Early symbolic music analysis environment; basis for subsequent MIR dataset handling.</td>
        <td style="padding:10px;">Eerola, T., & Toiviainen, P. (2004). *MIDI Toolbox: MATLAB tools for music research*. University of Jyväskylä.</td>
      </tr>
      <tr>
        <td style="padding:10px;">Huang, C. A., & Yang, Y.-H. (Academia Sinica)</td>
        <td style="padding:10px;">POP909 Dataset</td>
        <td style="padding:10px;">909 full pop songs in aligned MIDI format for melody, harmony, and structure analysis.</td>
        <td style="padding:10px;">Wang, Z. et al. (2020). *POP909: A pop-song dataset for music arrangement generation*. ISMIR.</td>
      </tr>
      <tr>
        <td style="padding:10px;">Choi, K. et al. (Spotify Research)</td>
        <td style="padding:10px;">GiantMIDI-Piano</td>
        <td style="padding:10px;">Automated transcription of piano music into large-scale MIDI dataset for generative models.</td>
        <td style="padding:10px;">Kong, Q. et al. (2020). *GiantMIDI-Piano: A large-scale MIDI dataset for classical piano music*. ISMIR.</td>
      </tr>
    </tbody>
  </table>
</section>


  <p>This real-time approach </p>
</div>


    <div class="background-reading" id="background">
        <h3>Background Reading</h3>
        <ul>
            <li><strong>Deep Listening and Creative Awareness</strong><br/>
                Oliveros, P. (2005). <em>Deep Listening: A Composer’s Sound Practice</em>. iUniverse.<br/>
                A foundational philosophy for this project, framing listening as a creative, meditative, and relational practice.
            </li>

            <li><strong>Real-Time Music Information Retrieval (MIR)</strong><br/>
                Fiebrink, R. (2011). <em>Real-Time Human Interaction with Supervised Learning Algorithms for Music Composition and Performance</em> (PhD Thesis, Princeton).<br/>
                Explores adaptive, performer-in-the-loop systems for real-time learning — a precursor to the live analysis in Deep Listening.<br/>
                Lartillot, O., et al. (2008). <em>A MATLAB Toolbox for Music Information Retrieval</em>. Proc. ISMIR.<br/>
                Early example of feature extraction frameworks in MIR, forming the basis for real-time adaptation in creative contexts.
            </li>

            <li><strong>Computational Creativity and Collaboration</strong><br/>
                Bown, O., & Martin, A. (2019). <em>Machine Listening: Improvisation with Algorithms</em>. Organised Sound.<br/>
                Discusses how algorithmic systems can participate in improvisation without deterministic control — central to Deep Listening’s feedback philosophy.<br/>
                Collins, N. (2008). <em>The Analysis of Generative Music Programs</em>. Organised Sound.<br/>
                Highlights the tension between generative autonomy and human co-creation; informs this project’s balance between analysis and performer agency.
            </li>

            <li><strong>Real-Time Pattern Recognition and Deep Learning</strong><br/>
                Hawthorne, C., et al. (2018). <em>Onsets and Frames: Dual-Objective Piano Transcription</em>. ISMIR.<br/>
                Demonstrates deep neural architectures for high-resolution MIDI transcription — a model for real-time symbolic representation.<br/>
                Choi, K., et al. (2017). <em>Towards Music Embeddings: Learning with Triplet Loss</em>. ISMIR.<br/>
                Provides methods for embedding musical sequences in high-dimensional latent spaces — relevant for identifying emergent meta-sequences in MIDI streams.
            </li>

            <li><strong>Human–Machine Improvisation Systems</strong><br/>
                Agres, K., & Herremans, D. (2020). <em>Music and Artificial Intelligence: From Composition to Performance</em>. Frontiers in Artificial Intelligence.<br/>
                Surveys how AI can engage with human musicians dynamically — a framing for real-time improvisation feedback models.<br/>
                Lewis, G. E. (2000). <em>Too Many Notes: Computers, Complexity, and Culture in Voyager</em>. Leonardo Music Journal.<br/>
                Describes the Voyager system, an early model for computer-human improvisation, philosophically aligned with Deep Listening’s interactive ethos.
            </li>
        </ul>
    </div>

    <div class="data-collection" id="data-collection">
        <h3>Setting Up the Data Collection Environment</h3>
        <p>Capturing rich, high-resolution musical data is central to the Deep Listening project. To track real-time decisions and interactions across multiple devices, the system relies on MIDI as the core information protocol, integrating controllers, pedals, and synthesizers into a unified data stream.</p>

        <h4>1. Devices and Signal Sources</h4>

        <h3>MIDI & Audio Routing Diagram</h3>
<pre style="font-family: monospace; background-color: #f7f7f7; padding: 1rem; overflow-x: auto;">
                   ┌─────────────────────┐
                   │  Morningstar        │
                   │     MC8 Pro         │
                   │ 4 Omniports + 5-pin │
                   └───────┬─────────────┘
                           │
       ┌───────────────────┼──────────────────────────────────────────────┐
       │                   │                           │                  │
       ▼                   ▼                           ▼                  ▼
Blooper [Ring-Active MIDI IN]  Cloudburst [Standard Type A MIDI IN]  Mood MK II [Ring-Active MIDI IN]  Lost & Found [Ring-Active MIDI IN]

                           │
                           ▼
Timeline [5-pin MIDI IN] ──▶ MIDI THRU (5-pin) ──▶ GT-1000 Core [3.5mm MIDI IN] ──▶ MIDI THRU (3.5mm) ──▶ Arturia MicroFreak [3.5mm MIDI IN]

──────────────────────────────────────────────────────────────
Audio Path
──────────────────────────────────────────────────────────────
[Guitar] ──▶ LS-2 Input
             ├─A Output──▶ FX Loop 1 ──▶ GT-1000 Core ──▶ Return A (LS-2)
             │
             └─B Output──▶ Sonic Cake ABY Box ──▶ FX Loop 2 ─▶ Timeline ─▶ Cloudburst ─▶ Mood MK II ─▶ Source Audio EQ2 ─▶ Return B (LS-2)

Arturia MicroFreak ───────────┘ (joined via Sonic Cake ABY Box for FX Loop 2)

LS-2 Output ──▶ Blooper ─▶ AER Alpha Amp
</pre>

        <p>The primary devices used for data collection include:</p>
        
<ul>
    <li>
        <strong>MC8 Pro (Patch/Scene Controller):</strong> Central hub coordinating device changes, presets, and scene-level commands. Sends and receives MIDI signals to control multiple connected devices simultaneously, ensuring all instruments and pedals stay in sync during performance.
    </li>
    <li>
        <strong>Chase Bliss Blooper (Looper/Effects):</strong> Captures real-time loops, gestures, and effect parameters via MIDI Continuous Controllers (CC). Enables granular control over layering, reverse/forward loops, and experimental sound textures.
    </li>
    <li>
        <strong>Korg Modwave (Synthesizer):</strong> A wavetable and sample-based synthesizer offering expressive multi-dimensional controls. Captures note events, modulation, velocity, aftertouch, and gestures for high-resolution sonic analysis.
    </li>
    <li>
        <strong>Cloudburst (Effects/Controller):</strong> Multi-effects unit with stereo delay, reverb, and modulation. Provides detailed MIDI feedback for expressive timing, modulation, and wet/dry signal blending.
    </li>
    <li>
        <strong>Mood MK II (Looper/Effects):</strong> Real-time looper with MIDI integration. Tracks timing, gesture, and loop length, allowing analysis of rhythmic structure and improvisational layering.
    </li>
    <li>
        <strong>Lost & Found (Looper/Effects):</strong> Provides ring-active MIDI input for loop recording and playback, capturing both live performance gestures and expressive pedal manipulation.
    </li>
    <li>
        <strong>Timeline (Looper/Sampler):</strong> Captures and sequences MIDI and audio loops with precise timing. Allows pattern-based performance analysis, including tempo, note density, and rhythmic variation.
    </li>
    <li>
        <strong>GT-1000 Core (Multi-FX Processor):</strong> Offers a wide range of effects, including amp modeling, modulation, and delay. MIDI input captures dynamic effects changes and expression data during performance.
    </li>
    <li>
        <strong>Arturia MicroFreak (Synthesizer):</strong> Digital oscillator synth with wavetable, FM, and virtual analog modes. Captures note events, pitch modulation, and expressive gestures for sonic and rhythmic pattern analysis.
    </li>
</ul>


        <p>Other devices can be added as the system scales, including modular synths, drum machines, and pedals with MIDI capability.</p>
    </div>


 <div class="related-code" id="related-code"> <h3>Related Code</h3> <p> This system connects the <strong>Morningstar MC8 Pro</strong> MIDI controller to a computer via USB, allowing Python to listen to and record all incoming MIDI messages in real time. Using the <code>mido</code> and <code>python-rtmidi</code> libraries, the script continuously monitors for MIDI input such as note events, control changes, and pedal actions. </p> <p> Each MIDI message is parsed and timestamped at the moment it arrives. These messages are then written into a local <strong>SQLite database</strong>, creating a structured record of the performance data that includes event type, channel, control value, and precise timing information. </p> <p> The system architecture follows a simple flow: </p> <div style="margin-left: 20px;"> <ol> <li>Initialize MIDI input from the MC8 Pro using the <code>mido</code> library.</li> <li>Listen continuously for incoming messages (e.g., Control Change, Program Change, Note On/Off).</li> <li>Parse and timestamp each message as it arrives.</li> <li>Insert the message data into an SQLite database in real time.</li> <li>Provide live feedback or visualization (optional extension for performance contexts).</li> </ol> </div> <p> This approach allows for <strong>non-destructive, continuous data collection</strong> during live performance — ideal for analyzing gesture patterns, timing, and creative interaction between devices. Because the entire process runs in real time, it enables responsive systems that can analyze or react to performance data as it happens. </p> <p> The complete implementation, including setup instructions and example scripts, is available on GitHub. </p> </div>


    <div id="links">
        <h3>Links & References</h3>
        <p>
            <a href="#">Project GitHub / Resources</a><br/>
            <a href="#">Related Publications</a>
        </p>
    </div>

    <figure>
        <figcaption>
            <!-- Optional extra info -->
        </figcaption>
    </figure>
</body>
</html>
