<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Deep Listening Project - Jamie Gabriel</title>
    <style>
        body {
            font-family: sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 2rem;
            line-height: 1.6;
            color: #222;
            background-color: #fff;
        }

        h1 {
            text-align: left;
        }

        /* Flex layout for image on right */
        .intro-flex {
            display: flex;
            align-items: flex-start;
            gap: 2rem;
            margin: 2rem 0;
        }

        .intro-text {
            flex: 1;
        }

        .intro-image img {
            width: 150px;
            height: auto;
            display: block;
        }

        figure {
            margin: 1rem 0; /* reduced spacing globally */
        }

        figure:first-of-type {
            margin-top: 0.5rem;
        }

        figcaption h3 {
            margin: 0.2rem 0;
            font-size: 1.2rem;
        }

        figcaption h3 a {
            color: #000;
            text-decoration: none;
        }

        figcaption h3 .small {
            font-size: 0.9rem;
            color: #666;
        }

        a {
            color: #007bff;
            text-decoration: none;
        }

        a:hover {
            text-decoration: underline;
        }

        .toc ul {
            list-style-type: disc;
            margin-left: 2rem;
        }
    </style>
</head>
<body>
    <h1>Deep Listening</h1>
    <h2>Real-Time MIDI Pattern Analysis for Assisted Improvisation</h2>
    <a class="back-link" href="./index.html">← Back</a>

    <!-- Table of Contents -->
    <div class="toc">
        <h3>Contents</h3>
        <ul>
            <li><a href="#description">Overview</a></li>
                <li><a href="#why-different">How is this different</a></li>
            <li><a href="#background">Background Reading</a></li>
            <li><a href="#data-collection">Setting up for data dollection</a></li>
                    <li><a href="#related-code">Related code</a></li>
            <li><a href="#links">Links & References</a></li>
        </ul>
    </div>

    <div class="intro-flex" id="description">


        <div class="intro-text">
             <h3>Overview</h3>
           

            <p>The idea of this project is to examine MIDI data that can be  captured in real-time,(whether it's from intruments, effects units or whatever) and undertake analysis on it to capture underlying structures in musical interaction. Once the analysis takes place, new MIDI back into equipment to enehance live performance. The MIDI data that is captured includes note data, rhythmic data, synth parameters, and instrument pedal data, anything that can use the MIDI protocal. Deep learning models applied and the new MIDI data is created in response to the analysis. </p>

            <p>This project is not about automating performance or mimicking the human ability to improvise and create in real-time. Rather, the idea here is to augment the improvisation experience it, find ways to make it more integrated into what is unfolding in live settings. When working in ensembles,  improvising musicians are in a constant process of perceiving their own and others’ musical decisions. This project is about finding ways to make that experience deeper and more immersive.</p>

            <p>The project brings together alot of the things that I am really interested in - improvisation, real-tme sound design, the use of advanced mathematics to augment the experience of collaboration, finding latent patterns in data. A lot of geeky stuff, sure, but the most important aspect of the project is to find ways to hear and experience music. It is not simply about pattern recognition for the sake finding patterns, it about creating ways to have richer musical interations.  </p>
            <p>
            The end goal is to find new ways to think about interactive creative performance, where data itself becomes a dynamic actor  dialogue between humans and the technolgy, leading to new creative experiences and outcomes.</p>
        </div>
    </div>

    <div class="why-different" id="why-different">
    	 <h3>How is this different?</h3>

  <p>In terms of the field this project is in, it is most closely allingned with Music Information Retrieval (MIR). I completed my PhD in this area a few years ago now (examing ways to search symbolic music data) nlike most MIDI analysis systems, which operate in post-hoc or offline modes, this project is designed for real-time interpretation and feedback. Traditional MIDI data studies focus on pattern mining, expressive timing, or style modeling after performance data has been captured. By contrast, this system listens and responds as the performance unfolds.</p>

  <p>The aim is not just to analyze musical data, but to augment creative awareness during performance. Every gesture — from a footswitch on the MC8 Pro to modulation sweeps on a Chase Bliss pedal or pitch manipulation on the Korg Modwave — is captured, timestamped, and analyzed live. These micro-decisions form a data stream that can be visualized or transformed into feedback, creating a dialogue between performer, instrument, and algorithm.</p>

  <p>This real-time approach reframes MIDI not as a recording protocol, but as a language of interaction. It enables a new kind of deep listening where performers can explore emergent patterns, rhythmic densities, and collaborative dynamics as they happen — turning data into an active participant in the creative process.</p>
</div>


    <div class="background-reading" id="background">
        <h3>Background Reading</h3>
        <ul>
            <li><strong>Deep Listening and Creative Awareness</strong><br/>
                Oliveros, P. (2005). <em>Deep Listening: A Composer’s Sound Practice</em>. iUniverse.<br/>
                A foundational philosophy for this project, framing listening as a creative, meditative, and relational practice.
            </li>

            <li><strong>Real-Time Music Information Retrieval (MIR)</strong><br/>
                Fiebrink, R. (2011). <em>Real-Time Human Interaction with Supervised Learning Algorithms for Music Composition and Performance</em> (PhD Thesis, Princeton).<br/>
                Explores adaptive, performer-in-the-loop systems for real-time learning — a precursor to the live analysis in Deep Listening.<br/>
                Lartillot, O., et al. (2008). <em>A MATLAB Toolbox for Music Information Retrieval</em>. Proc. ISMIR.<br/>
                Early example of feature extraction frameworks in MIR, forming the basis for real-time adaptation in creative contexts.
            </li>

            <li><strong>Computational Creativity and Collaboration</strong><br/>
                Bown, O., & Martin, A. (2019). <em>Machine Listening: Improvisation with Algorithms</em>. Organised Sound.<br/>
                Discusses how algorithmic systems can participate in improvisation without deterministic control — central to Deep Listening’s feedback philosophy.<br/>
                Collins, N. (2008). <em>The Analysis of Generative Music Programs</em>. Organised Sound.<br/>
                Highlights the tension between generative autonomy and human co-creation; informs this project’s balance between analysis and performer agency.
            </li>

            <li><strong>Real-Time Pattern Recognition and Deep Learning</strong><br/>
                Hawthorne, C., et al. (2018). <em>Onsets and Frames: Dual-Objective Piano Transcription</em>. ISMIR.<br/>
                Demonstrates deep neural architectures for high-resolution MIDI transcription — a model for real-time symbolic representation.<br/>
                Choi, K., et al. (2017). <em>Towards Music Embeddings: Learning with Triplet Loss</em>. ISMIR.<br/>
                Provides methods for embedding musical sequences in high-dimensional latent spaces — relevant for identifying emergent meta-sequences in MIDI streams.
            </li>

            <li><strong>Human–Machine Improvisation Systems</strong><br/>
                Agres, K., & Herremans, D. (2020). <em>Music and Artificial Intelligence: From Composition to Performance</em>. Frontiers in Artificial Intelligence.<br/>
                Surveys how AI can engage with human musicians dynamically — a framing for real-time improvisation feedback models.<br/>
                Lewis, G. E. (2000). <em>Too Many Notes: Computers, Complexity, and Culture in Voyager</em>. Leonardo Music Journal.<br/>
                Describes the Voyager system, an early model for computer-human improvisation, philosophically aligned with Deep Listening’s interactive ethos.
            </li>
        </ul>
    </div>

    <div class="data-collection" id="data-collection">
        <h3>Setting Up the Data Collection Environment</h3>
        <p>Capturing rich, high-resolution musical data is central to the Deep Listening project. To track real-time decisions and interactions across multiple devices, the system relies on MIDI as the core information protocol, integrating controllers, pedals, and synthesizers into a unified data stream.</p>

        <h4>1. Devices and Signal Sources</h4>

        <h3>MIDI & Audio Routing Diagram</h3>
<pre style="font-family: monospace; background-color: #f7f7f7; padding: 1rem; overflow-x: auto;">
                   ┌─────────────────────┐
                   │  Morningstar        │
                   │     MC8 Pro         │
                   │ 4 Omniports + 5-pin │
                   └───────┬─────────────┘
                           │
       ┌───────────────────┼──────────────────────────────────────────────┐
       │                   │                           │                  │
       ▼                   ▼                           ▼                  ▼
Blooper [Ring-Active MIDI IN]  Cloudburst [Standard Type A MIDI IN]  Mood MK II [Ring-Active MIDI IN]  Lost & Found [Ring-Active MIDI IN]

                           │
                           ▼
Timeline [5-pin MIDI IN] ──▶ MIDI THRU (5-pin) ──▶ GT-1000 Core [3.5mm MIDI IN] ──▶ MIDI THRU (3.5mm) ──▶ Arturia MicroFreak [3.5mm MIDI IN]

──────────────────────────────────────────────────────────────
Audio Path
──────────────────────────────────────────────────────────────
[Guitar] ──▶ LS-2 Input
             ├─A Output──▶ FX Loop 1 ──▶ GT-1000 Core ──▶ Return A (LS-2)
             │
             └─B Output──▶ Sonic Cake ABY Box ──▶ FX Loop 2 ─▶ Timeline ─▶ Cloudburst ─▶ Mood MK II ─▶ Source Audio EQ2 ─▶ Return B (LS-2)

Arturia MicroFreak ───────────┘ (joined via Sonic Cake ABY Box for FX Loop 2)

LS-2 Output ──▶ Blooper ─▶ AER Alpha Amp
</pre>

        <p>The primary devices used for data collection include:</p>
        
<ul>
    <li>
        <strong>MC8 Pro (Patch/Scene Controller):</strong> Central hub coordinating device changes, presets, and scene-level commands. Sends and receives MIDI signals to control multiple connected devices simultaneously, ensuring all instruments and pedals stay in sync during performance.
    </li>
    <li>
        <strong>Chase Bliss Blooper (Looper/Effects):</strong> Captures real-time loops, gestures, and effect parameters via MIDI Continuous Controllers (CC). Enables granular control over layering, reverse/forward loops, and experimental sound textures.
    </li>
    <li>
        <strong>Korg Modwave (Synthesizer):</strong> A wavetable and sample-based synthesizer offering expressive multi-dimensional controls. Captures note events, modulation, velocity, aftertouch, and gestures for high-resolution sonic analysis.
    </li>
    <li>
        <strong>Cloudburst (Effects/Controller):</strong> Multi-effects unit with stereo delay, reverb, and modulation. Provides detailed MIDI feedback for expressive timing, modulation, and wet/dry signal blending.
    </li>
    <li>
        <strong>Mood MK II (Looper/Effects):</strong> Real-time looper with MIDI integration. Tracks timing, gesture, and loop length, allowing analysis of rhythmic structure and improvisational layering.
    </li>
    <li>
        <strong>Lost & Found (Looper/Effects):</strong> Provides ring-active MIDI input for loop recording and playback, capturing both live performance gestures and expressive pedal manipulation.
    </li>
    <li>
        <strong>Timeline (Looper/Sampler):</strong> Captures and sequences MIDI and audio loops with precise timing. Allows pattern-based performance analysis, including tempo, note density, and rhythmic variation.
    </li>
    <li>
        <strong>GT-1000 Core (Multi-FX Processor):</strong> Offers a wide range of effects, including amp modeling, modulation, and delay. MIDI input captures dynamic effects changes and expression data during performance.
    </li>
    <li>
        <strong>Arturia MicroFreak (Synthesizer):</strong> Digital oscillator synth with wavetable, FM, and virtual analog modes. Captures note events, pitch modulation, and expressive gestures for sonic and rhythmic pattern analysis.
    </li>
</ul>


        <p>Other devices can be added as the system scales, including modular synths, drum machines, and pedals with MIDI capability.</p>
    </div>


 <div class="related-code" id="related-code"> <h3>Related Code</h3> <p> This system connects the <strong>Morningstar MC8 Pro</strong> MIDI controller to a computer via USB, allowing Python to listen to and record all incoming MIDI messages in real time. Using the <code>mido</code> and <code>python-rtmidi</code> libraries, the script continuously monitors for MIDI input such as note events, control changes, and pedal actions. </p> <p> Each MIDI message is parsed and timestamped at the moment it arrives. These messages are then written into a local <strong>SQLite database</strong>, creating a structured record of the performance data that includes event type, channel, control value, and precise timing information. </p> <p> The system architecture follows a simple flow: </p> <div style="margin-left: 20px;"> <ol> <li>Initialize MIDI input from the MC8 Pro using the <code>mido</code> library.</li> <li>Listen continuously for incoming messages (e.g., Control Change, Program Change, Note On/Off).</li> <li>Parse and timestamp each message as it arrives.</li> <li>Insert the message data into an SQLite database in real time.</li> <li>Provide live feedback or visualization (optional extension for performance contexts).</li> </ol> </div> <p> This approach allows for <strong>non-destructive, continuous data collection</strong> during live performance — ideal for analyzing gesture patterns, timing, and creative interaction between devices. Because the entire process runs in real time, it enables responsive systems that can analyze or react to performance data as it happens. </p> <p> The complete implementation, including setup instructions and example scripts, is available on GitHub. </p> </div>


    <div id="links">
        <h3>Links & References</h3>
        <p>
            <a href="#">Project GitHub / Resources</a><br/>
            <a href="#">Related Publications</a>
        </p>
    </div>

    <figure>
        <figcaption>
            <!-- Optional extra info -->
        </figcaption>
    </figure>
</body>
</html>
