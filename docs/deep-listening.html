<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Deep Improvisation Project - Jamie Gabriel</title>
    <style>
        body {
            font-family: sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 2rem;
            line-height: 1.6;
            color: #222;
            background-color: #fff;
        }

        h1 {
            text-align: left;
        }

        /* Flex layout for image on right */
        .intro-flex {
            display: flex;
            align-items: flex-start;
            gap: 2rem;
            margin: 2rem 0;
        }

        .intro-text {
            flex: 1;
        }

        .intro-image img {
            width: 150px;
            height: auto;
            display: block;
        }

        figure {
            margin: 1rem 0; /* reduced spacing globally */
        }

        figure:first-of-type {
            margin-top: 0.5rem;
        }

        figcaption h3 {
            margin: 0.2rem 0;
            font-size: 1.2rem;
        }

        figcaption h3 a {
            color: #000;
            text-decoration: none;
        }

        figcaption h3 .small {
            font-size: 0.9rem;
            color: #666;
        }

        a {
            color: #007bff;
            text-decoration: none;
        }

        a:hover {
            text-decoration: underline;
        }

        .toc ul {
            list-style-type: disc;
            margin-left: 2rem;
        }
    </style>
</head>
<body>
    <h1>Deep Improvisation</h1>
    <h2>Real-Time MIDI Pattern Analysis for Augmented Improvisation</h2>
    <a class="back-link" href="./index.html">← Back</a>

    <!-- Table of Contents -->
    <div class="toc">
        <h3>Contents</h3>
        <ul>
            <li><a href="#description">Overview</a></li>
                <li><a href="#why-different">How is this different</a></li>
            <li><a href="#background">Some background reading</a></li>
            <li><a href="#data-collection">Example architecture</a></li>
             <li><a href="#get-involved">Getting involved in this project</a></li>

                    <li><a href="#related-code">Related code</a></li>
            <li><a href="#links">Links & References</a></li>
        </ul>
    </div>

    <div class="intro-flex" id="description">


        <div class="intro-text">
             <h3>Overview</h3>
           

          <p>The core idea of this project is to capture and analyse MIDI data in real time — whether from instruments, effects units, or controllers — to uncover the underlying structures of musical interaction. Each performance gesture, such as modulation sweeps on a Chase Bliss pedal or pitch manipulation on the Korg Modwave, is recorded, timestamped, and analysed live. These micro-decisions form a continuous data stream. This can then be visualised or transformed into responsive feedback to the performer, instrument, and evolving underlying deep learingmodel. I am aiming to capture all available MIDI information, including note, rhythmic, and synthesis parameters, as well as pedal and controller data, anything that can communicate via the MIDI protocol. Deep learning models are then applied to interpret these streams and generate new MIDI data in response, enriching and enhancing the live performance.</p>


            <p>This project is not about automating performance or mimicking the human ability to improvise and create in real-time. Rather, the idea here is to augment the improvisation experience it, find ways to make it more integrated into what is unfolding in live settings. When working in ensembles,  improvising musicians are in a constant process of perceiving their own and others’ musical decisions. This project is about finding ways to make that experience deeper and more immersive.</p>

            <p>The project brings together alot of the things that I am really interested in - improvisation, real-tme sound design, the use of advanced mathematics to augment the experience of collaboration, finding latent patterns in data. A lot of geeky stuff, sure, but the most important aspect of the project is to find ways to hear and experience music. It is not simply about pattern recognition for the sake finding patterns, it about creating ways to have richer musical interations.  </p>
            <p>
            The end goal is to find new ways to think about interactive creative performance, reframes MIDI not as a recording protocol, but as a language of interaction. It enables a new kind of deep listening where performers can explore emergent patterns, rhythmic densities, and collaborative dynamics as they happen — turning data into an active participant in the creative process. where data itself becomes a dynamic actor  dialogue between humans and the technolgy, leading to new creative experiences and outcomes.</p>

            <p>
                The title, Deep Listening is a nod to Pauline Oliveros' work in this field FIX..
            </p>
        </div>
    </div>

    <div class="why-different" id="why-different">
    	 <h3>How is this different?</h3>

  <p>There are a number of groups especially in the Music Information Retrieval (MIR) space, working with MIDI data. Most of them explore how MIDI can be used in post-hoc or offline modes. Traditional MIDI data studies focus on pattern mining, expressive timing, or style modeling after performance data has been captured. By contrast, this system listens and responds as the performance unfolds.This project this project is designed for real-time interpretation and feedback. </p>

  <p>The aim is not just to analyze musical data, but to augment creative awareness during performance. Every gesture — from a footswitch on the MC8 Pro to 



  </p>

  <section id="ismir-midi-research">
  <h2>Key ISMIR Researchers and Projects on MIDI Datasets</h2>
  <p>This table summarises leading contributions from the ISMIR community in developing and analysing MIDI datasets for music information retrieval (MIR), machine learning, and computational creativity research.</p>

  <table style="width:100%; border-collapse:collapse; margin-top:1em;">
    <thead>
      <tr style="background-color:#f0f0f0; text-align:left;">
        <th style="padding:10px; border-bottom:2px solid #ccc;">Researcher / Group</th>
        <th style="padding:10px; border-bottom:2px solid #ccc;">Project / Dataset</th>
        <th style="padding:10px; border-bottom:2px solid #ccc;">Focus / Contribution</th>
        <th style="padding:10px; border-bottom:2px solid #ccc;">Reference (APA7)</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td style="padding:10px;">Hawthorne, C. (Google Magenta)</td>
        <td style="padding:10px;">MAESTRO Dataset</td>
        <td style="padding:10px;">High-quality aligned audio–MIDI data from piano performances for transcription and generation tasks.</td>
        <td style="padding:10px;">Hawthorne, C. et al. (2018). *Enabling factorized piano music modeling and generation with the MAESTRO dataset*. ISMIR.</td>
      </tr>
      <tr>
        <td style="padding:10px;">Simon, I., Roberts, A. (Google Magenta)</td>
        <td style="padding:10px;">PerformanceRNN / Magenta MIDI Dataset</td>
        <td style="padding:10px;">Expressive performance modeling and real-time sequence generation using recurrent neural networks.</td>
        <td style="padding:10px;">Oore, S. et al. (2018). *This time with feeling: Learning expressive musical performance*. Neural Computing and Applications.</td>
      </tr>
      <tr>
        <td style="padding:10px;">Donahue, C. (UCSD)</td>
        <td style="padding:10px;">MIDI-DDSP</td>
        <td style="padding:10px;">Neural synthesis conditioned on MIDI control parameters for expressive sound generation.</td>
        <td style="padding:10px;">Huang, R., & Donahue, C. (2021). *MIDI-DDSP: Detailed control of musical performance via hierarchical modeling*. ISMIR.</td>
      </tr>
      <tr>
        <td style="padding:10px;">Raffel, C. (Columbia / Google)</td>
        <td style="padding:10px;">Lakh MIDI Dataset</td>
        <td style="padding:10px;">Large-scale collection of MIDI files aligned with Million Song Dataset for symbolic–audio mapping.</td>
        <td style="padding:10px;">Raffel, C. (2016). *Learning-based methods for comparing sequences, with applications to audio-to-MIDI alignment and matching*. PhD thesis, Columbia University.</td>
      </tr>
      <tr>
        <td style="padding:10px;">Kim, J., & Bello, J. P. (NYU)</td>
        <td style="padding:10px;">URMP Dataset</td>
        <td style="padding:10px;">Synchronized audio–video–MIDI dataset of chamber ensembles for multi-modal music analysis.</td>
        <td style="padding:10px;">Li, B., Kim, J., & Bello, J. P. (2018). *URMP: A dataset for multimodal music performance analysis*. ISMIR.</td>
      </tr>
      <tr>
        <td style="padding:10px;">MIDI Toolbox (Eerola & Toiviainen)</td>
        <td style="padding:10px;">MATLAB MIDI Toolbox</td>
        <td style="padding:10px;">Early symbolic music analysis environment; basis for subsequent MIR dataset handling.</td>
        <td style="padding:10px;">Eerola, T., & Toiviainen, P. (2004). *MIDI Toolbox: MATLAB tools for music research*. University of Jyväskylä.</td>
      </tr>
      <tr>
        <td style="padding:10px;">Huang, C. A., & Yang, Y.-H. (Academia Sinica)</td>
        <td style="padding:10px;">POP909 Dataset</td>
        <td style="padding:10px;">909 full pop songs in aligned MIDI format for melody, harmony, and structure analysis.</td>
        <td style="padding:10px;">Wang, Z. et al. (2020). *POP909: A pop-song dataset for music arrangement generation*. ISMIR.</td>
      </tr>
      <tr>
        <td style="padding:10px;">Choi, K. et al. (Spotify Research)</td>
        <td style="padding:10px;">GiantMIDI-Piano</td>
        <td style="padding:10px;">Automated transcription of piano music into large-scale MIDI dataset for generative models.</td>
        <td style="padding:10px;">Kong, Q. et al. (2020). *GiantMIDI-Piano: A large-scale MIDI dataset for classical piano music*. ISMIR.</td>
      </tr>
    </tbody>
  </table>
</section>


  <p>This real-time approach </p>
</div>


    <div class="background-reading" id="background">
        <h3>Background Reading</h3>
        <ul>
            <li><strong>Deep Listening and Creative Awareness</strong><br/>
                Oliveros, P. (2005). <em>Deep Listening: A Composer’s Sound Practice</em>. iUniverse.<br/>
                A foundational philosophy for this project, framing listening as a creative, meditative, and relational practice.
            </li>

            <li><strong>Real-Time Music Information Retrieval (MIR)</strong><br/>
                Fiebrink, R. (2011). <em>Real-Time Human Interaction with Supervised Learning Algorithms for Music Composition and Performance</em> (PhD Thesis, Princeton).<br/>
                Explores adaptive, performer-in-the-loop systems for real-time learning — a precursor to the live analysis in Deep Listening.<br/>
                Lartillot, O., et al. (2008). <em>A MATLAB Toolbox for Music Information Retrieval</em>. Proc. ISMIR.<br/>
                Early example of feature extraction frameworks in MIR, forming the basis for real-time adaptation in creative contexts.
            </li>

            <li><strong>Computational Creativity and Collaboration</strong><br/>
                Bown, O., & Martin, A. (2019). <em>Machine Listening: Improvisation with Algorithms</em>. Organised Sound.<br/>
                Discusses how algorithmic systems can participate in improvisation without deterministic control — central to Deep Listening’s feedback philosophy.<br/>
                Collins, N. (2008). <em>The Analysis of Generative Music Programs</em>. Organised Sound.<br/>
                Highlights the tension between generative autonomy and human co-creation; informs this project’s balance between analysis and performer agency.
            </li>

            <li><strong>Real-Time Pattern Recognition and Deep Learning</strong><br/>
                Hawthorne, C., et al. (2018). <em>Onsets and Frames: Dual-Objective Piano Transcription</em>. ISMIR.<br/>
                Demonstrates deep neural architectures for high-resolution MIDI transcription — a model for real-time symbolic representation.<br/>
                Choi, K., et al. (2017). <em>Towards Music Embeddings: Learning with Triplet Loss</em>. ISMIR.<br/>
                Provides methods for embedding musical sequences in high-dimensional latent spaces — relevant for identifying emergent meta-sequences in MIDI streams.
            </li>

            <li><strong>Human–Machine Improvisation Systems</strong><br/>
                Agres, K., & Herremans, D. (2020). <em>Music and Artificial Intelligence: From Composition to Performance</em>. Frontiers in Artificial Intelligence.<br/>
                Surveys how AI can engage with human musicians dynamically — a framing for real-time improvisation feedback models.<br/>
                Lewis, G. E. (2000). <em>Too Many Notes: Computers, Complexity, and Culture in Voyager</em>. Leonardo Music Journal.<br/>
                Describes the Voyager system, an early model for computer-human improvisation, philosophically aligned with Deep Listening’s interactive ethos.
            </li>
        </ul>
    </div>

    <div class="data-collection" id="data-collection">
        <h3>Example Architecture</h3>
        <p>Capturing rich, high-resolution musical data is central to the Deep Listening project. To track real-time decisions and interactions across multiple devices, the system relies on MIDI as the core information protocol, integrating controllers, pedals, and synthesizers into a unified data stream.</p>

        <h4>1. Devices and Signal Sources</h4>

        <h3>MIDI & Audio Routing Diagram</h3>
<pre style="font-family: monospace; background-color: #f7f7f7; padding: 1rem; overflow-x: auto;">
                   ┌─────────────────────┐
                   │  Morningstar        │
                   │     MC8 Pro         │
                   │ 4 Omniports + 5-pin │
                   └───────┬─────────────┘
                           │
       ┌───────────────────┼──────────────────────────────────────────────┐
       │                   │                           │                  │
       ▼                   ▼                           ▼                  ▼
Blooper [Ring-Active MIDI IN]  Cloudburst [Standard Type A MIDI IN]  Mood MK II [Ring-Active MIDI IN]  Lost & Found [Ring-Active MIDI IN]

                           │
                           ▼
Timeline [5-pin MIDI IN] ──▶ MIDI THRU (5-pin) ──▶ GT-1000 Core [3.5mm MIDI IN] ──▶ MIDI THRU (3.5mm) ──▶ Arturia MicroFreak [3.5mm MIDI IN]

──────────────────────────────────────────────────────────────
Audio Path
──────────────────────────────────────────────────────────────
[Guitar] ──▶ LS-2 Input
             ├─A Output──▶ FX Loop 1 ──▶ GT-1000 Core ──▶ Return A (LS-2)
             │
             └─B Output──▶ Sonic Cake ABY Box ──▶ FX Loop 2 ─▶ Timeline ─▶ Cloudburst ─▶ Mood MK II ─▶ Source Audio EQ2 ─▶ Return B (LS-2)

Arturia MicroFreak/Octotrack/Korg Modwave ───────────┘ (joined via Sonic Cake ABY Box for FX Loop 2)

LS-2 Output ──▶ Blooper ─▶ AER Alpha Amp
</pre>




    </div>


<div class="get-involved" id="get-involved"> </div>
<h3>Getting involved in this project</h3>

Coming soon


<div class="related-code" id="related-code">
    <h3>Related Code</h3>

    <p>
        The <strong>Deep Improvisation Project</strong> is supported by several interconnected codebases that together form the foundation for real-time analysis, interaction, and visualization. These repositories allow the system to capture performance data, run machine learning models live, render dynamic visual feedback, and enable distributed collaboration across networked musicians.
    </p>

    <p>
        <strong>1. Python (MIDI + Deep Learning)</strong><br/>
        Python scripts handle all <strong>MIDI messaging and real-time data management</strong>. Built with <code>mido</code> and <code>python-rtmidi</code>, this environment listens continuously for incoming messages — including note, CC, and control data — and records them with microsecond precision into an <strong>SQLite database</strong>. Deep learning models (e.g., PyTorch, TensorFlow) can then process these streams to predict or generate new patterns, supporting <em>live adaptive improvisation</em>.
    </p>

    <p>
        <strong>2. SuperCollider (Performance Engine)</strong><br/>
        The SuperCollider layer manages <strong>sound generation and performance interaction</strong>. It receives interpreted data from the Python process and responds with synthesized gestures, rhythmic patterns, and evolving textures. This setup enables non-linear improvisation structures, linking analytical and auditory domains in real time.
    </p>

    <p>
        <strong>3. Node.js (Visualisation Layer)</strong><br/>
        A Node.js environment provides <strong>interactive visual feedback</strong> using JavaScript libraries such as <code>D3.js</code> and <code>Three.js</code>. It displays rhythmic lattices, gesture maps, and timing densities, offering musicians a visual interface to explore the evolving structure of improvisations. The visualiser connects directly to the Python data stream for low-latency rendering.
    </p>

    <p>
        <strong>4. Jamulus (Low-Latency Collaboration Config)</strong><br/>
        Configuration files define <strong>Jamulus-based network setups</strong> for online improvisation and rehearsal. These allow distributed musicians to connect to the Deep Improvisation environment with minimal latency, integrating the analytical and visual systems into remote collaborative performance.
    </p>

    <p>
        Each of these repositories contributes to a modular, extensible framework for <strong>real-time creative AI and human–machine improvisation</strong>. The complete implementations, setup instructions, and example scripts are hosted across four dedicated GitHub repositories:
    </p>

    <div style="margin-left: 20px;">
        <ul>
            <li><a href="#">Deep Improvisation – Python MIDI + ML</a></li>
            <li><a href="#">Deep Improvisation – SuperCollider Performance</a></li>
            <li><a href="#">Deep Improvisation – Node.js Visualisation</a></li>
            <li><a href="#">Deep Improvisation – Jamulus Network Configs</a></li>
        </ul>
    </div>

    <p>
        These systems together form the technical and conceptual backbone of the Deep Improvisation project — a live, evolving conversation between <strong>data, sound, and human creativity</strong>.
    </p>
</div>



    <div id="links">
        <h3>Links & References</h3>
        <p>
            <a href="#">Project GitHub / Resources</a><br/>
            <a href="#">Related Publications</a>
        </p>
    </div>

    <figure>
        <figcaption>
            <!-- Optional extra info -->
        </figcaption>
    </figure>
</body>
</html>
