<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Deep Listening Project - Jamie Gabriel</title>
    <style>
        body {
            font-family: sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 2rem;
            line-height: 1.6;
            color: #222;
            background-color: #fff;
        }

        h1 {
            text-align: left;
        }

        .intro-flex {
            display: flex;
            align-items: flex-start;
            gap: 2rem;
            margin: 2rem 0;
        }

        .intro-text {
            flex: 1;
        }

        .intro-image img {
            width: 150px;
            height: auto;
            display: block;
        }

        figure {
            margin: 1rem 0;
        }

        figure:first-of-type {
            margin-top: 0.5rem;
        }

        figcaption h3 {
            margin: 0.2rem 0;
            font-size: 1.2rem;
        }

        figcaption h3 a {
            color: #000;
            text-decoration: none;
        }

        figcaption h3 .small {
            font-size: 0.9rem;
            color: #666;
        }

        a {
            color: #007bff;
            text-decoration: none;
        }

        a:hover {
            text-decoration: underline;
        }

        .toc ul {
            list-style-type: disc;
            margin-left: 2rem;
        }
    </style>
</head>
<body>
    <h1>Deep Improv</h1>
    <h2>Real-Time MIDI and Spectrogram Pattern Analysis for Augmented Improvisation</h2>
    <a class="back-link" href="./index.html">← Back</a>

    <!-- Table of Contents -->
    <div class="toc">
        <h3>Contents</h3>
        <ul>
            <li><a href="#description">Overview</a></li>
            <li><a href="#background">Some background reading</a></li>
            <li><a href="#data-collection">Example architecture and related code</a></li>
            <li><a href="#get-involved">Getting involved in this project</a></li>
        </ul>
    </div>

    <div class="intro-flex" id="description">
        <div class="intro-text">
            <h3>Overview</h3>

            <p>
                The purpose of this project is to capture and analyse MIDI data in real time and use it to enhance live improvised performance. The data includes both note-on/note-off events as well as any changes that might be taking place in MIDI-enabled effects units (for example, values coming from an LFO filter knob on a synth, or a level change on a guitar pedal). 
                All data is recorded, timestamped, and analysed using real-time deep learning models. These models are used to create new MIDI data which can be fed back to performers as musical stimulus during improvisation.
            </p>

            <p>
                The goal is not to automate performance or simulate human improvisers. Instead, it is to enhance the improvisational environment by allowing performers to interact with latent musical structures as they emerge — a real-time feedback loop between musician, data, and sound.
            </p>

            <p>
                This is a multidisciplinary project combining improvisation practice, live sound design, computational modelling, and symbolic music representation. It explores how performers can engage with high-resolution, continuously evolving musical data, creating a form of collaboration in which the system becomes a responsive participant in the musical dialogue.
            </p>


            

            <p>
                Finally, the name <strong>Deep Improv</strong> is a nod to <i>Deep Listening</i>, a life long project undertaken by Pauline Oliveros that explored expanded listening practices and the creative, collaborative relational potential of sound. This project aims extends that lineage into live improvisation using MIDI and spectrogram data created in real time.
            </p>
        </div>
    </div>


    <div class="background-reading" id="background">
        <h3>Background Reading</h3>
        <p>
            You can find some background reading, kind of an ongoing lit review, as interesting related work comes out <a href="./background-reading.html">here</a>
        </p>




    </div>



    <div class="data-collection" id="data-collection">
        <h3>Related code</h3>

 <!--        <p>
            The diagram below captures one possible setup of MIDI & audio routing currently used in this project. There are many possible variations; this is just the working environment used for data collection and live experimentation.
        </p> -->
<!-- 
<pre style="font-family: monospace; background-color: #f7f7f7; padding: 1rem; overflow-x: auto;">
                   ┌─────────────────────┐
                   │  Morningstar        │
                   │     MC8 Pro         │
                   │ 4 Omniports + 5-pin │
                   └───────┬─────────────┘
                           │
       ┌───────────────────┼──────────────────────────────────────────────┐
       │                   │                           │                  │
       ▼                   ▼                           ▼                  ▼
Blooper [Ring-Active MIDI IN]  Cloudburst [Standard Type A MIDI IN]  Mood MK II [Ring-Active MIDI IN]  Lost & Found [Ring-Active MIDI IN]

                           │
                           ▼
Timeline [5-pin MIDI IN] ──▶ MIDI THRU (5-pin) ──▶ GT-1000 Core [3.5mm MIDI IN] ──▶ MIDI THRU (3.5mm) ──▶ Arturia MicroFreak [3.5mm MIDI IN]

──────────────────────────────────────────────────────────────
Audio Path
──────────────────────────────────────────────────────────────
[Guitar] ──▶ LS-2 Input
             ├─A Output──▶ FX Loop 1 ──▶ GT-1000 Core ──▶ Return A (LS-2)
             │
             └─B Output──▶ Sonic Cake ABY Box ──▶ FX Loop 2 ─▶ Timeline ─▶ Cloudburst ─▶ Mood MK II ─▶ Source Audio EQ2 ─▶ Return B (LS-2)

Arturia MicroFreak/Octotrack/Korg Modwave ───────────┘ (joined via Sonic Cake ABY Box for FX Loop 2)

LS-2 Output ──▶ Blooper ─▶ AER Alpha Amp
</pre>
 -->
        <p>
            This project is supported by several interconnected codebases that work together to collect data, process it in real time, generate responsive musical output, and visualise emerging patterns.
        </p>

        <p>
            <strong>1. Setup and data management </strong><br/>
            <a href="#">Link to related code</a><br/>
            A collection of starting scripts, mostly in Python. Setting up things like receiving and sending MIDI and Spectrogram data from different sound sources, writing this data to databases etc. Also some handy scripts and config to set up remote performance environments (such as Jamulus)
        </p>

        <p>
            <strong>2. Real-time performance templates and ML</strong><br/>
            <a href="#">Link to related code</a><br/>
            Code to manage and create feedback in real-time performance environments (using SuperCollider, MAX) also integrated with Python ML models
        </p>

        <p>
            <strong>3. Visualisation (Node.js)</strong><br/>
            <a href="#">Link to related code</a><br/>
            Mostly javascript code, to manage realtime visualisation (using D3.js, Three.js) of music data. 
        </p>

   
    </div>

    <div class="get-involved" id="get-involved"></div>
    <h3>Getting involved in this project</h3>

    <p>To get involved in this project, reach out to me at <a href="mailto:jamie@jamiegabriel.org">jamie@jamiegabriel.org</a></p>

    <figure>
        <figcaption></figcaption>
    </figure>
</body>
</html>
