<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Deep Listening Project - Jamie Gabriel</title>
    <style>
        body {
            font-family: sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 2rem;
            line-height: 1.6;
            color: #222;
            background-color: #fff;
        }

        h1 {
            text-align: left;
        }

        .intro-flex {
            display: flex;
            align-items: flex-start;
            gap: 2rem;
            margin: 2rem 0;
        }

        .intro-text {
            flex: 1;
        }

        .intro-image img {
            width: 150px;
            height: auto;
            display: block;
        }

        figure {
            margin: 1rem 0;
        }

        figure:first-of-type {
            margin-top: 0.5rem;
        }

        figcaption h3 {
            margin: 0.2rem 0;
            font-size: 1.2rem;
        }

        figcaption h3 a {
            color: #000;
            text-decoration: none;
        }

        figcaption h3 .small {
            font-size: 0.9rem;
            color: #666;
        }

        a {
            color: #007bff;
            text-decoration: none;
        }

        a:hover {
            text-decoration: underline;
        }

        .toc ul {
            list-style-type: disc;
            margin-left: 2rem;
        }
    </style>
</head>
<body>
    <h1>Deep Listening</h1>
    <h2>Real-Time MIDI Pattern Analysis for Augmented Improvisation</h2>
    <a class="back-link" href="./index.html">← Back</a>

    <!-- Table of Contents -->
    <div class="toc">
        <h3>Contents</h3>
        <ul>
            <li><a href="#description">Overview</a></li>
            <li><a href="#background">Some background reading</a></li>
            <li><a href="#data-collection">Example architecture and related code</a></li>
            <li><a href="#get-involved">Getting involved in this project</a></li>
        </ul>
    </div>

    <div class="intro-flex" id="description">
        <div class="intro-text">
            <h3>Overview</h3>

            <p>
                The purpose of this project is to capture and analyse MIDI data in real time and use it to enhance live improvised performance. The data includes both note-on/note-off events as well as any changes that might be taking place in MIDI-enabled effects units (for example, values coming from an LFO filter knob on a synth, or a level change on a guitar pedal). 
                All data is recorded, timestamped, and analysed using real-time deep learning models. These models are used to create new MIDI data which can be fed back to performers as musical stimulus during improvisation.
            </p>

            <p>
                The goal is not to automate performance or simulate human improvisers. Instead, it is to enhance the improvisational environment by allowing performers to interact with latent musical structures as they emerge — a real-time feedback loop between musician, data, and sound.
            </p>

            <p>
                This is a multidisciplinary project combining improvisation practice, live sound design, computational modelling, and symbolic music representation. It explores how performers can engage with high-resolution, continuously evolving musical data, creating a form of <em>deep listening</em> in which the system becomes a responsive participant in the musical dialogue.
            </p>

            <p>
                The name <strong>Deep Listening</strong> references Pauline Oliveros’ lifelong work exploring expanded listening practices and the creative, relational potential of sound. This project extends that lineage into real-time algorithmic improvisation.
            </p>
        </div>
    </div>


    <div class="background-reading" id="background">
        <h3>Background Reading</h3>
        <p>
            Below is a consolidated selection of foundational texts and contemporary research relevant to the Deep Listening project. These works span deep listening practice, computational creativity, real-time MIR, and live human–machine improvisation.
        </p>

<section id="deep-listening-lit">


    <!-- OLIVEROS -->
    <div class="lit-block">
        <b>Deep Listening & Sonic Awareness</b>
        <p><strong>Oliveros, P. (2005).</strong> <em>Deep Listening: A Composer’s Sound Practice</em>. iUniverse.</p>
        <p>
            Oliveros provides the foundational philosophical framework for this project: listening as an
            active, relational, and ethical practice rather than passive hearing. Her emphasis on
            attentiveness, reciprocity, and co-presence directly informs how algorithmic agents in
            performance should behave—when to respond, when to hold back, and how to maintain musical
            agency and respect for human partners.
        </p>
    </div>

    <!-- FIEBRINK -->
    <div class="lit-block">
        <b>Real-Time MIR, Interactive Machine Learning & Performer-in-the-Loop Systems</b>

        <p><strong>Fiebrink, R. (2011).</strong> <em>Real-Time Human Interaction with Supervised Learning
            Algorithms for Music Composition and Performance</em>. Princeton University.</p>
        <p>
            Fiebrink’s thesis is a landmark in interactive machine learning for musicians. The work
            demonstrates how performers can actively train, refine, and reshape ML models during
            performance, enabling adaptive mappings and personalised behaviours. This directly underpins
            your Deep Listening system’s design ethos: systems should learn with the performer, not just
            before the performance.
        </p>

        <p><strong>Lartillot, O., Toiviainen, P., & Eerola, T. (2008).</strong> A MATLAB Toolbox for Music
            Information Retrieval. <em>Data Analysis, Machine Learning and Applications</em>.</p>
        <p>
            Offers the conceptual foundation for MIR feature extraction. Although your system uses
            real-time implementations, the vocabulary of features (onsets, chroma, spectral flux,
            centroids) and their musical interpretations traces back to this lineage.
        </p>
    </div>

    <!-- IMPROVISATION / CREATIVITY -->
    <div class="lit-block">
        <b>Computational Creativity & Algorithmic Improvisation</b>

        <p><strong>Bown, O., & Martin, A. (2019).</strong> Machine Listening: Improvisation with Algorithms.
            <em>Organised Sound</em>.</p>
        <p>
            Explores algorithmic improvisers as co-creative agents, offering concrete design considerations
            for making systems “feel musical.” Emphasizes evaluation, responsiveness, and human-centred
            interaction—all central to Deep Listening.
        </p>

        <p><strong>Collins, N. (2008).</strong> The Analysis of Generative Music Programs.
            <em>Organised Sound, 13</em>(3).</p>
        <p>
            Provides taxonomy and critical frameworks for generative systems, including autonomy levels,
            randomness control, and interpretive transparency. Helps contextualise how your system should
            generate events, respond to performers, and expose control to users.
        </p>
    </div>

    <!-- DEEP LEARNING -->
    <div class="lit-block">
        <b>Deep Learning for Symbolic & Real-Time Pattern Recognition</b>

        <p><strong>Hawthorne, C., et al. (2018).</strong> Onsets and Frames: Dual-Objective Piano Transcription.
            <em>ISMIR / arXiv</em>.</p>
        <p>
            Demonstrates high-fidelity symbolic representation derived from audio, forming the conceptual
            basis for real-time symbolic interaction. Onsets–Frames models are useful for detecting performer
            intent, expressive gesture, and timing information that can feed predictive models.
        </p>

        <p><strong>Choi, K., Fazekas, G., & Sandler, M. (2017).</strong> Towards Music Embeddings: Learning with
            Triplet Loss. <em>ISMIR</em>.</p>
        <p>
            Shows how embedding spaces can encode short musical phrases for similarity, retrieval, or
            conditioning. Embeddings become essential tools in your system for:
        </p>

        <div class="sub-points">
            <p>• Real-time motif matching</p>
            <p>• Predictive modelling</p>
            <p>• Corpus-driven improvisation</p>
        </div>
    </div>

    <!-- HUMAN-MACHINE IMPROVISATION -->
    <div class="lit-block">
        <b>Human–Machine Improvisation Systems</b>

        <p><strong>Agres, K., & Herremans, D. (2020).</strong> Music and Artificial Intelligence: From Composition
            to Performance. <em>Frontiers in Artificial Intelligence</em>.</p>
        <p>
            A state-of-the-art overview of AI-supported creative systems. This serves as the larger academic
            context in which your Deep Listening system situates itself—connecting ML, improvisation, and
            performance research.
        </p>

        <p><strong>Lewis, G. E. (2000).</strong> Too Many Notes: Computers, Complexity, and Culture in Voyager.
            <em>Leonardo Music Journal</em>.</p>
        <p>
            The Voyager system is the historical prototype for fully autonomous improvising agents. Lewis’s
            framing—non-hierarchical human–machine interplay, distributed agency, and computational “musical
            personalities”—is essential theoretical grounding for any modern interactive system.
        </p>
    </div>

    <!-- PROTOCOLS -->
    <div class="lit-block">
        <h3>Real-Time Protocols, Latency, and Expressive Control</h3>

        <p><strong>Wessel, D., & Wright, M. (2002).</strong> Problems and Prospects for Intimate Musical Control
            of Computers. <em>Computer Music Journal</em>.</p>
        <p>
            Focuses on latency, gesture mapping, and multimodal control — topics central to designing responsive,
            expressive live systems. Their insights directly shape your system’s technical constraints
            (e.g., keeping round-trip latency below ~15–30 ms for ensemble-tight interactions).
        </p>

        <p><strong>Wright, M., & Freed, A. (1997).</strong> Open Sound Control: A New Protocol for Communicating
            with Sound Synthesizers. <em>ICMC</em>.</p>
        <p>
            Outlines OSC’s advantages over MIDI: higher resolution, network friendliness, address hierarchies,
            and time-tagged bundles. These are essential in your setup where MIDI controllers, a Raspberry Pi,
            SuperCollider, and Python ML models communicate in real time.
        </p>
    </div>

</section>


    </div>



    <div class="data-collection" id="data-collection">
        <b>Example architecture and related code</b>

        <p>
            The diagram below captures one possible setup of MIDI & audio routing currently used in this project. There are many possible variations; this is just the working environment used for data collection and live experimentation.
        </p>
<!-- 
<pre style="font-family: monospace; background-color: #f7f7f7; padding: 1rem; overflow-x: auto;">
                   ┌─────────────────────┐
                   │  Morningstar        │
                   │     MC8 Pro         │
                   │ 4 Omniports + 5-pin │
                   └───────┬─────────────┘
                           │
       ┌───────────────────┼──────────────────────────────────────────────┐
       │                   │                           │                  │
       ▼                   ▼                           ▼                  ▼
Blooper [Ring-Active MIDI IN]  Cloudburst [Standard Type A MIDI IN]  Mood MK II [Ring-Active MIDI IN]  Lost & Found [Ring-Active MIDI IN]

                           │
                           ▼
Timeline [5-pin MIDI IN] ──▶ MIDI THRU (5-pin) ──▶ GT-1000 Core [3.5mm MIDI IN] ──▶ MIDI THRU (3.5mm) ──▶ Arturia MicroFreak [3.5mm MIDI IN]

──────────────────────────────────────────────────────────────
Audio Path
──────────────────────────────────────────────────────────────
[Guitar] ──▶ LS-2 Input
             ├─A Output──▶ FX Loop 1 ──▶ GT-1000 Core ──▶ Return A (LS-2)
             │
             └─B Output──▶ Sonic Cake ABY Box ──▶ FX Loop 2 ─▶ Timeline ─▶ Cloudburst ─▶ Mood MK II ─▶ Source Audio EQ2 ─▶ Return B (LS-2)

Arturia MicroFreak/Octotrack/Korg Modwave ───────────┘ (joined via Sonic Cake ABY Box for FX Loop 2)

LS-2 Output ──▶ Blooper ─▶ AER Alpha Amp
</pre>
 -->
        <p>
            This project is supported by several interconnected codebases that work together to collect data, process it in real time, generate responsive musical output, and visualise emerging patterns.
        </p>

        <p>
            <strong>1. Data collection and analysis (Python)</strong><br/>
            <a href="#">Link to related code</a><br/>
            Python scripts using <code>mido</code> and <code>python-rtmidi</code> capture MIDI data with microsecond precision into an SQLite database. Deep learning models (PyTorch/TensorFlow) process streams to generate predictions, embeddings, or new musical material during live performance.
        </p>

        <p>
            <strong>2. Real-time performance templates (SuperCollider)</strong><br/>
            <a href="#">Link to related code</a><br/>
            SuperCollider receives interpreted data and controls synthesis, gesture responses, and algorithmic textures, enabling real-time interaction between performer and system.
        </p>

        <p>
            <strong>3. Visualisation (Node.js)</strong><br/>
            <a href="#">Link to related code</a><br/>
            JavaScript visualisers (D3.js, Three.js) display rhythmic lattices, gesture trajectories, and emergent structures. These connect directly to the Python processes for live rendering.
        </p>

        <p>
            <strong>4. Jamulus (Network Collaboration)</strong><br/>
            <a href="#">Link to related code</a><br/>
            Jamulus configurations enable distributed improvisation with low-latency feedback, integrating Deep Listening tools into remote performance setups.
        </p>
    </div>

    <div class="get-involved" id="get-involved"></div>
    <h3>Getting involved in this project</h3>

    <p>Coming soon</p>

    <figure>
        <figcaption></figcaption>
    </figure>
</body>
</html>
