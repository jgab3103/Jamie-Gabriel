{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>STATUS: Draft<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sympy as sp\n",
    "import random  as rd\n",
    "from IPython.display import IFrame\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HTM Overview 1: Introduction to HTM, Bit Arrays & SDRs</h2>\n",
    "\n",
    "Now that we have things all set up, let's start looking at HTM itself. For now, my general approach will be to follow along with the HTM School Videos. These are a fantastic resource, and I will also point to some other handy resources along the way. We will also build a number of the visualisations that appear in the HTM School Videos using Python to help gain an intuitition. \n",
    "\n",
    "Let's start at <b>Episode 0: HTM Overview</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IFrame(\"https://www.youtube.com/embed/XMB0ri4qgwc\", 600, 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This provides a really nice high level introduction to HTM. No coding is happening here. Some things to note from this video: \n",
    "\n",
    "* HTM is all about understanding how exactly the neocortex works and using this knowledge to build machines and solve problems. The neocortex is the sheet of cells that wraps around the top of the brain, and accounts for around 75% of the brain's mass. It has evolved in mammals, particularly humans. In alot of HTM resources, it is often described as an approximately 2.5mm thick, a kind of scrunched up sheet, inside the human skull, with similiar height and width to a dinner napkin. While there are many functions that human's undertake (such as eatings/sleeping/sex etc.) that are controlled from other parts of the brain, the neocortex holds things such as memories, a notion of identity, intelligence. The neocortex is sometimes referred to as the \"seat of intelligence\". \n",
    "\n",
    "* If you examine the neocortex under a microscope, it turns out that its cellular structure in different regions is almost identical. This suggests to us that however the neocortex might work, it seems to utilise a similar approach to processing regardless of different types of information it receives (such as visual, auditory, touch etc.). To put this another way, when the neocortex processes information, it appears to use the same algorithmic structure.\n",
    "\n",
    "* The neocortex is made up of a <i>lot</i> of cells. In fact, an average human adult neocortex contains 20-30 billion cells, and the predominant type of cell is called a neuron. You can think of a neuron as a kind of 2-state machine, that is one of two states at any one time, and the states are defined by its level of electric voltage. A neuron will either be around -70 millivolts (which we can regard as being in an 'off' state), or is around 100 millivolts (which we can regard as being as being in an 'on' or 'firing' state). At any point time (leaving aside the issue that its a little problematic to define exactly what we mean by point in time) all neurons in the neocortext can be regarded as being in a state of either 'on' or 'off'.\n",
    "\n",
    "* It also turns out that there is a remarkably small percentage of cells in the 'on' state at any time (usually only around 2% of the neurons). Some kind of notion of sparsity seems is critical in how the neocortex functions.\n",
    "\n",
    "* The information that comes into the neocortex to be processed can be broadly seen in two categories: \n",
    "\n",
    "1. <i>Basic inputs that arrive into the neocortex which are a copy of the motor commands coming out of the old brain</i> <br/> These inputs are a copy of information that has been sent to control muscles, and comes into the neocortex in order for it to undertand how the body is interacting with the world\n",
    "2. <i>Sensory input coming form the outside world</i><br/> This includes sensory inputs from touch, smell etc, from phenomena occuring in the outside world, all copied and sent into the neocortex, and comprising a representation of what is happening in the world at a given time. Cummulatively, this provides a sensory motor model, and allows the neocortex to have continuity in the way it experiences phenomena. \n",
    "\n",
    "This episode of HTM School also touches on the kinds of things that need to be in place if we were to attempt to create some kind of system that could function in an equivalent way to the neocortex. First, we would require a mechanism to convert incoming information into a format that the neocortex can deal with. The system would need a mechanism to convert information into a data structure that can be used. In HTM, we will end up creating a mechanism to do this called <b>Encoder</b>. The data structure that information is converted will be called a <b>Sparse Distributed Representation</b> or SDR. \n",
    "\n",
    "To make this all work, we would also need some mechanism to process multiple interacting SDRs, to somehow bring them all together into some kind of shared and malleable representation. In HTM, this mechanism will be called a <b>Spatial Pooler</b>. And finally, we will need a mechanism to understand and predict sequences of events so, like the neocortex, we might infer what what is about to happen, or reason about something based on information we have encountered. In HTM, this will be called the <b>Temporal Memory</b> Algorithm. \n",
    "\n",
    "To paraphrase all this, we might say that the purpose of a HTM system is to receive sensory information, encode it into an SDR structure, create a high level representation that takes into account the information across other SDRs using a Spatial Pooler, and allow the creation of predictive states and knowledge through a Temporal Memory Algorithm. This will be a system that is designed to learn continuously where there is no notion of a training set, no batch processing, and the model of the world can change as the data changes. This allows the HTM system to build predictive model of the world, and continously adapt. \n",
    "\n",
    "This episode introduces some specific terms in neuroscience, and it is probably a good idea to check out some related resources on this. For a good starting point, see the <i>Brains Explained</i> YouTube channel, especially the videos covering neuron basics (https://www.youtube.com/watch?v=eAbIWSPS1M4), the neocortex https://www.youtube.com/watch?v=x2mYTaJPVnc&t=511s, and the thalumus (https://www.youtube.com/watch?v=fki7AmLma_I). This will give you some initial intuitions around what neurons are (which are certainly far more complex than my description as a 2-state machine above), and how the neocortex is structured, and how information flows in this structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep on going with the HTM School videos, and now look at <b>Episode 1: Bit Arrays</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IFrame(\"https://www.youtube.com/embed/Ub1fE-bAroA\", 600, 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Episode 1 in the HTM School series is not so much focused on HTM, rather it is a primer for working with bit array's, and encoding messages in strings of binary numbers. If you work across the computer science space where this comes up quite a bit, the advice is that this video could be skipped. Still, I still found it quite handy, and some things to note in this video are:  \n",
    "\n",
    "* A bit here refers to a variable which can be used to encode the smallest atomic piece of information that can be stored in a computer: a variable that is set to either 1 or 0. This variable encodes a binary number, which can take on only 2 values, either 0 or 1. \n",
    "\n",
    "* You can think of the 0 or 1 as two states that a bit can be in (like an 'on/firing' or 'off' state in a neuron). Note that when bits are spoken about, different domain specific types of nomenclature are usually used: 1 or 0, on or off, true of false, etc. Whatever language is used, the value of bit tells us about its state in regard to the possible values it can take on. \n",
    "\n",
    "* It is possible to have an ordered list of bits, which you might visualise as something like <b>00110011</b>. This is called a <b>Bit Array</b>\n",
    "\n",
    "* Bit Arrays are well suited to encode information. To demonstrate, this, consider that, if I have an 8 bit array and I and I can choose whether each bit could be in a state of 0 or 1, I would able create 256 unique 8 bit arrays (e.g., 00000000, 00000001, 00000011, etc). The math behind this is straight forward $2^8 = 256$, and we are simply saying that there are 256 ways to arrange 8 values where each value can have a 0 or 1. \n",
    "\n",
    "* There are some basic operations that can take place with bit arrays such as <b>AND</b>, <b>OR</b> and <b>NOT</b>. Each of these can be thought of a function that takes two bit arrays, and returns one bit array. There is also a <b>NOT</b> operation that takes 1 bit as an input and returns 1 bit. We can think of these functions as: \n",
    "\n",
    "<br/><b>AND (bitArray1, bitArray2)</b>:<br/>\n",
    "Takes two bit arrays as args, and returns a bit array where that has only those bits where 1s appear in both. The result is also called an <i>intersection</i> of two given bit arrays <br/>\n",
    "<i>Example</i><br/>\n",
    "$ \\text{AND}(00110011, 00111000) = 00110000 $\n",
    "\n",
    "<br/><b>OR (bitArray1, bitArray2)</b>:<br/>\n",
    "Takes two bit arrays as args, and returns a bit array where the bits those bits where 1s appear in bitArray1 or bitArray2. The result is also called an <i>union</i> of two given bit arrays <br/>\n",
    "<i>Example</i><br/>\n",
    "$ \\text{OR}(10110011, 01110011) = 11111011 $\n",
    "\n",
    "\n",
    "<br/><b>XOR (bitArray1, bitArray2)</b>:<br/>\n",
    "Takes two bit arrays as args, and returns a bit array where each of the corresponding bits, must be bitArray1 or bitArray2. The result is also called an <i>union</i> of two given bit arrays <br/>\n",
    "<i>Example</i><br/>\n",
    "$ \\text{OR}(10110011, 01110011) = 11000000 $\n",
    "\n",
    "\n",
    "<br/><b>NOT (bitArray1)</b>:<br/>\n",
    "Takes two bit arrays as args, and returns a bit array where <br/>\n",
    "<i>Example</i><br/>\n",
    "$ \\text{NOT}(00110011) = 11001100 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the nomenclature that tends to surround bit arrays, (which can include but is not limited to bits, bytes, binary numbers, booleans, logic gates, etc, which are all kind of the same but not really), can make what should be straightforward a little opaque at times. For our immediete purposes, we are only going to be concerned with AND and OR operations, but the deeper we dig into HTM and the critical SDR data structure, we will encounter issues related to to some of these operations, and we will need to explore the underlying mathematics a little more. \n",
    "\n",
    "To gain a deeper understanding of all this, and the best resource I have come across is  Norman Wildberger's YouTube series on Boolean Logic: <a href=\"https://www.youtube.com/watch?v=UcjKKcvv7Rw&list=PLIljB45xT85CnIGIWb7tH1F_S2PyOC8rb\">https://www.youtube.com/watch?v=UcjKKcvv7Rw&list=PLIljB45xT85CnIGIWb7tH1F_S2PyOC8rb</a>, that brings the various way that all of this exists across different displines and puts all under a rigorous lens of linear algebra. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that primer our of the way, let's move on to <b>Episode 2: SDR Capacity & Comparison</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IFrame(\"https://www.youtube.com/embed/ZDgCdWTuIzc\", 600, 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Episode 2, we start to get into more specific ideas that underpin HTM systems. Matt does a really cool opening to this video, discussing the experience of considering what happens when we play a musical istrument. He discusses the the complex feedback mechanism that allows the neocortex to deal with recieving input information (such as auditory and motor input) and how it is involved in also planning the music you are about to play, and how this information is fed to into the various regions of the neocortex. This kind of task, listening to and playing music, is a really nice example of something that is just so complicated, and yet the neocortex can make it work.\n",
    "\n",
    "Some important things to note in this video: \n",
    "\n",
    "* It introduces some important nomenclature around the characteristics of bit arrays. The first of these is <b>population</b>, which denotes the count of how many non-zero values are in a given bit array, and <b>size</b> refers to the total count of bits in a bit array. In HTM, population is often denoted as $w$ and size is often denoted as $n$. As an example, the bit array 00110011 has a population of 4 and size of 8 ($w = 4, n = 8$). \n",
    "\n",
    "* The term population is also synonomous with the <b>active bits</b> of a bit array. This term of 'active bits' seems to predominate through the related HTM literature. This concept is also related to the 'hamming weight', though this does not really turn up in the related literature.\n",
    "\n",
    "* The number of active bits in a given bit array is of critical importance in HTM. For example, 00111111 has more active bits (or a greater population) than 00000011. This difference is often qualitatively described in terms of a measure of sparsity (meaning a bit-array having few active bits realtive to its size) or density (a bit array having many active bits relative to its seize) to describe bit arrays with predominantly zeros or  predominantly ones. A simple quanatitative metric is also associated with this, which is called a measure of sparsity, being the percentage of active bits in an array: \n",
    "<br/>\n",
    "$$ \\frac{\\text{active bits}}{\\text{size}} \\times 100$$\n",
    "<br/>\n",
    "So for example, in a bit array of size 256, where 6 bits are active (where $w=6$ and $n=256$), this sparsity metric is just over $2%$. Given such a percentage of active bits, we would say it is a sparse array.  \n",
    "<br/>\n",
    "\n",
    "* The idea of sparsity in HTM is such an important idea that the term bit array is usually reframed into the concept of a <b>Sparse Distributed Array</b>, meaning a bit array which has the characteristic of having a low number of active bits relative to its size. This data structure plays a big part in HTM, being used across multiple parts of the underlying HTM algorithms. \n",
    "\n",
    "* There is a important notion of semantic similiarity that exists between given SDRs, which will also become a critical idea for us as we move forward. We say that an SDR is more similiar to another SDR, when they shares same value (either a 0 or 1) in the same position. of of the SDRs. For example, an SDR comprised of the bits 00000001 and a second SDR comprised of the bits 00000001 are semantically identical. As a second example, the SDR  000000001 is more semantically similiar to a second SDR 00000011 than it is to the SDR 11110011, because the first and second SDRs have more instances of the same value (either 0 or 1) at each position. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is straightforward to create this type of structure in Python. Here is an example of an SDR with $n = 256$, and $w = 8$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros(256)\n",
    "a[248:] = 1\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extend our intuition around all this, let's build one of the HTM School visualisations. It will give us more of a sense of how it works, and we can use it future HTM school videos.\n",
    "\n",
    "To do this, first let's write some code create an SDR. Note that I am going to build a class for this, because an SDR is an object that should be able to exhibit attributes such as input space size, number of active bits, etc, and using an array like the one above does not capture some of this nuance. Also, its alot of overhead to create an object like the one above, with all those zeros. It makes more sense to create a smaller list which would just include those indices across the array that have been set to 1. Finally, I will also allow the class to denote a label for the SDR which be handy later on: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRandomisedSdr(size, numberOfActiveBits):\n",
    "    sdr = set()\n",
    "    while len(sdr) < numberOfActiveBits:\n",
    "        sdr.add(np.random.randint(low=1, high=size))\n",
    "\n",
    "    return(list(sdr))\n",
    "\n",
    "class SDR:\n",
    "    def __init__(self, size, numberOfActiveBits, label):\n",
    "        self.size = size\n",
    "        self.numberOfActiveBits= numberOfActiveBits\n",
    "        self.activeBits = createRandomisedSdr(self.size, self.numberOfActiveBits)\n",
    "        self.label = label\n",
    "    def getSummary(self):\n",
    "        print(\"----------------- SUMMARY -------------------------\")\n",
    "        print(\"|L1| Label:\", self.label)\n",
    "        print(\"|L1| Input space size of SDR:\", self.size)\n",
    "        print(\"|L2| Number of active bits in SDR:\", self.numberOfActiveBits)\n",
    "        print(\"|L3| Percentage of active bits:\", (self.numberOfActiveBits / self.size) * 100, \"%\")\n",
    "        print(\"|L3| Active bits:\", self.activeBits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out, and print a summary of what is created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 1024\n",
    "active_bits = 32\n",
    "\n",
    "s1 = SDR(size, active_bits, \"SDR1\")\n",
    "s1.getSummary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a few visualisation function, just to take this list of indices that each represent active bits and convert to a coordinates that I could plot with Matplotlib: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_multiples(n):\n",
    "    result = set()\n",
    "    for i in range(1, int(n ** 0.5) + 1):\n",
    "        div, mod = divmod(n, i)\n",
    "        if mod == 0:\n",
    "            result |= {i, div}\n",
    "    result_as_list = list(result)\n",
    "    result_as_list.sort()\n",
    "    return(result_as_list)\n",
    "\n",
    "def compute_middle_factors(n): \n",
    "    if n < 20:\n",
    "        print(\"The chosen SDR size is too small and does not make sense to visualize in this way\")\n",
    "        return\n",
    "    result = compute_multiples(n)\n",
    "    \n",
    "    is_prime = len(result) == 2\n",
    "    if is_prime:\n",
    "        print(\"Prime number SDR sizes not supported\")\n",
    "        return\n",
    "    if len(result) < 5:\n",
    "        print(\"There are only\" , len(result), \"factors of\", n, \" so the dimensions grid will be unbalanced. Better to choose a different sdr_size\")\n",
    "    if sp.ntheory.primetest.is_square(n):\n",
    "        dim_one = result[sp.floor(len(result) / 2)]\n",
    "        dim_two = dim_one\n",
    "    else:\n",
    "        dim_one = result[sp.floor(len(result) / 2) - 1]\n",
    "        dim_two = result[sp.floor(len(result) / 2)]\n",
    "\n",
    "\n",
    "    return [dim_one, dim_two]\n",
    "\n",
    "\n",
    "\n",
    "def convert_sdr_to_tuple_for_visualisation(sdr, sdr_size): \n",
    "    counting_offset = 1\n",
    "    m = np.zeros(sdr_size)\n",
    "    for i in sdr:\n",
    "        m[i] = m[i] + 1\n",
    "    \n",
    "    dimensions = compute_middle_factors(sdr_size)\n",
    "\n",
    "    if dimensions == None:\n",
    "        return\n",
    "    d = np.reshape(m, [int(v) for v in dimensions])\n",
    "    v = np.where(d == 1)\n",
    "    \n",
    "    \n",
    "    coords = [(v[1][i], (dimensions[1] - counting_offset) - v[0][i]) for i in range(len(v[1]))]\n",
    "\n",
    "    \n",
    "    return(coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pass our SDR into this, and get some coordinates to plot. Here are first 5 coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = convert_sdr_to_tuple_for_visualisation(s1.activeBits, sdr_size)\n",
    "s2[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's plot it. We will also create a function to format our axis in Matplotlib: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_axis_for_sdr(ax, x_limit, y_limit, population, label, create_label = True):\n",
    "    \n",
    "    if create_label:\n",
    "        label_add = np.round((population / (x_limit * y_limit)) * 100, 2)\n",
    "        label = label + ' (Sparsity: {}%)'.format(label_add)\n",
    "        ax.set_xlabel(label)\n",
    "        \n",
    "    ax.set_xticks(range(int(x_limit)))\n",
    "    ax.set_yticks(range(int(y_limit)))\n",
    "    [ax.xaxis.get_major_ticks()[i].tick1line.set_color(\"white\") for i in range(int(x_limit))]\n",
    "    [ax.yaxis.get_major_ticks()[i].tick1line.set_color(\"white\") for i in range(int(y_limit))]\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.grid(color='k', linestyle='-', linewidth=.5)\n",
    "    ax.tick_params(axis = \"both\", which = \"both\", bottom = False, top = False)\n",
    "\n",
    "    \n",
    "    return(ax)\n",
    "\n",
    "\n",
    "fig, (ax1) = plt.subplots(1, 1, figsize=(6, 6))\n",
    "ax1 = create_axis_for_sdr(ax1, np.sqrt(sdr_size) + 1, np.sqrt(sdr_size) + 1, active_bits, s1.label)\n",
    "SDR1 = [ax1.add_patch(Rectangle(s2[i], 1, 1, color=\"blue\")) for i in range(len(s2))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what is this visualisation telling us? It displays an SDR that have been randomly intitialised with just under $3\\%$ of active bits. If I were to create this again, I would just get some other random SDR.\n",
    "\n",
    "One way to think of this, is that this SDR created with random bits, could represent some arbitrary piece of information. If I create a second SDR, I can use that to represent some other arbitrary piece of information. Note however that we have also introduced a semantic dimension to all this, and this set up we are saying that similiar SDRs (those wich have alot of bit values which are the same) are actually semantically similiar. \n",
    "\n",
    "A natural question that arises from all this is around <b>capacity</b>. In the context of the above example, this asks, how many ways are there to arrange 32 active bits and an SDR of size of 1024. This would tell me how many pieces of information this SDR could hold hold. \n",
    "\n",
    "This is a straight forward question question combinatorics, and this number is also known the <b>binomial coeffient</b>, and turns up everywhere in mathematics. It can be denoted as:  \n",
    "\n",
    "$$\\binom{n}{w}$$ \n",
    "\n",
    "Let's use the SymPy library in Python to calculate an example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.binomial(4, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice inutition for this is to imagine you have a game board with a grid of four spaces. You have two discs that can place in any combination across the 4 spaces. How many different combinations are there? That would be 6. \n",
    "\n",
    "As an aside, remember above when we introduced bit arrays and I said there \"if I have an 8 bit array and I and I can choose whether each bit could be in a state of 0 or 1, I would able create 256 unique 8 bit arrays (e.g., 00000000, 00000001, 00000011, etc)\"? In terms of the binomial coeffiecient, this what I am doing is seeing all the ways I could arrange 0 things, plus all the ways I could arrange 1 thing, plus all the ways I could arrange 2 things, and so on (giving me all the possible combinations). The computation for this would be: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.binomial(8, 0) + sp.binomial(8, 1) + sp.binomial(8, 2) + sp.binomial(8, 3) + sp.binomial(8, 4) + sp.binomial(8, 5) + sp.binomial(8, 6) + sp.binomial(8, 7) + sp.binomial(8, 8)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The thing to note about all this is that, as the number of spaces grows, this number will get big really quickly. Consider if we used the specifications above, and wanted to know how many ways I could arrange 32 things in 1024 spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.binomial(1024, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However we might articulate this number (and maybe the best way is just via scientific notation as $4.975934516475092e+60$) it is clear that for us it is a big number, and not really even imaginable. It's not quite the number of atoms in the known universe (which one could arguably put around $10^{78}$) but it is big, relative to how humans conceive numbers. \n",
    "\n",
    "Generally in HTM, its normal to work with a size up to $n = 65000$ with a $2\\%$ capacity (let's say approximately 1300). So if we did this, how many unique SDRs might we be able to create?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(sp.binomial(65000, 1300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this number is big, so big its kind of meaningless to us, and in this case is certainly bigger than the number of atoms in the known universe. But this also tells us about the amazing capacity SDRs have in storing discrete pieces of information. Of course, the composition of these discrete pieces of information is just the location of ones and zeros, and eventually we will need to deal with how to translate that into the kinds of more information that our neocortex seems to be able to cope with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Comparing SDRs</b>\n",
    "\n",
    "SDRs become far more interesting when we start to consider them in comparison with each other. So let's say I choose 2 randomly created SDRs out of all the SDRs I cold possible choose (that really big number above), and I compare these 2 SDRs in some way. Later, we will look at some more complicated comparison metrics but for now let's just consider how the 2 SDRs I have chosen may relate to each other through which of their active bits are the same, or <b>overlap</b>. The overlap metric that tells me how many active bits that two SDRs have in common. I will do this be creating a function that tells all the bits that are the same (which is the AND() function above, also called the interesection) in both SDRs. I will also build in to this fucntion an OR() operation (also called the union) as we will need it soon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeUnionAndOverlap(SDR1_on_bits, SDR2_on_bits):\n",
    "    union = list(set(SDR1_on_bits).union(SDR2_on_bits))\n",
    "    overlap = list(set(SDR1_on_bits).intersection(SDR2_on_bits))\n",
    "    \n",
    "    return({\"union\": union, \"overlap\": overlap})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out. I will create 2 randomonly initialised SDRs, and a third that computes thier union and overlap, and then plot everything. For the purposes of showing union and overlap, I will push the sparsity up to just under $10\\%$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 1024\n",
    "activeBits = 100\n",
    "\n",
    "s1 = SDR(size, activeBits, \"SDR1\")\n",
    "s2 = SDR(size, activeBits, \"SDR2\")\n",
    "s3 = computeUnionAndOverlap(s1.activeBits, s2.activeBits)\n",
    "\n",
    "v1 = convert_sdr_to_tuple_for_visualisation(s1.activeBits, sdr_size)\n",
    "v2 = convert_sdr_to_tuple_for_visualisation(s2.activeBits, sdr_size)\n",
    "v3 = convert_sdr_to_tuple_for_visualisation(s3['union'], sdr_size)\n",
    "v4 = convert_sdr_to_tuple_for_visualisation(s3['overlap'], sdr_size)\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "ax1 = create_axis_for_sdr(ax1, np.sqrt(size) + 1, np.sqrt(size) + 1, len(s1.activeBits), \"SDR 1\")\n",
    "ax2 = create_axis_for_sdr(ax2, np.sqrt(size) + 1, np.sqrt(size) + 1, len(s2.activeBits), \"SDR 2\")\n",
    "ax3 = create_axis_for_sdr(ax3, np.sqrt(size) + 1, np.sqrt(size) + 1, len(s3['union']), \"Union and Overlap (\" + str(len(s3['overlap'])) + \")\")\n",
    "SDR1 = [ax1.add_patch(Rectangle(v1[i], 1, 1, color=\"blue\")) for i in range(len(v1))]\n",
    "SDR2 = [ax2.add_patch(Rectangle(v2[i], 1, 1, color=\"blue\")) for i in range(len(v2))]\n",
    "SDR3 = [ax3.add_patch(Rectangle(v3[i], 1, 1, color=\"gray\")) for i in range(len(v3))]\n",
    "SDR1 = [ax3.add_patch(Rectangle(v4[i], 1, 1, color=\"orange\")) for i in range(len(v4))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first and second SDRs have been randomly initialised. The third, shows the union of both SDRs togehter (the squares in gray) and and active bit overlaps (in orange). As we might expect, the overall sparsity has increased and total number of active bits is now just under $18\\%$. \n",
    "\n",
    "We are mostly interested in this orange squares, that display the active bits that overlap from the the first and second SDR. We call the count of orange squares the <b>overlap score</b> and that is listed in the label in brackets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, this comparison is only being made between two SDRs out of an awful lot of other other possible SDRs above (for this chosen size of 1024 and number of active bits set at 100, there are around $7.746624668043452e+140$). It is worth checking to see if this particularly overlap score is a good representation of how SDRs in the size space might generally overlap. Let's do this by doing same thing over 100,000 iterations, just to get a sense of how our overlap scores are moving around. Note that if you are executing the code in the notebook, this will take a while:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createAndCompareSdrsOverMultipleIterations(iterations, sdr_size, population):\n",
    "    sdr_unions_for_comparison = []\n",
    "    sdr_overlaps_for_comparison = []\n",
    "    for x in range(iterations):\n",
    "        SDR1 = SDR(size, activeBits, \"SDR1\")\n",
    "        SDR2 = SDR(size, activeBits, \"SDR2\")\n",
    "        sdr_comparison = computeUnionAndOverlap(s1.activeBits, s2.activeBits)\n",
    "        sdr_unions_for_comparison.append(len(sdr_comparison['union']))\n",
    "        sdr_overlaps_for_comparison.append(len(sdr_comparison['overlap']))\n",
    "    print(\"Average union: \", str(sum(sdr_unions_for_comparison) / len(sdr_unions_for_comparison)))\n",
    "    print(\"Average overlap: \", str(sum(sdr_overlaps_for_comparison) / len(sdr_overlaps_for_comparison)))\n",
    "    return({\"union_comparison\": sdr_unions_for_comparison, \"overlap_comparison\": sdr_overlaps_for_comparison})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons = create_and_compare_sdrs_over_multiple_iterations(100000, 1024, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this looks like promising. After running it 100,000 times we get an overlap score average that is pretty close to the first one we did. The function also returned all the overlap scores and we can plot them as a histogram too: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax1.set_xlabel(\"Histogram showing counts of union of bits\")\n",
    "ax2.set_xlabel(\"Histogram showing counts of overlaps of bits\")\n",
    "p1 = ax1.hist(comparisons['union_comparison'])\n",
    "p2 = ax2.hist(comparisons['overlap_comparison'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this feels like what we would expect, and is really just a consequence of the binomial coefficient - there are more ways to get random bits that come toward some number close to 9, and the overall numbers will be a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Matching SDRs</b><br/>\n",
    "\n",
    "In HTM, the idea of a <b>match</b> between 2 SDRs is when they have some count of overlapping bits, based on some <b>match threshhold</b>, and arbitrary number that we set. So let's say we have 2 SDRs and set a match threshold at .9. This would mean that if $90\\%$ of the active bits in both SDRS overlap, we have a match . Or perhaps we might set this at .1, meaning that if $10\\%$ of active bits in both SDRs overlap, we have a match. Let's create a function to do this:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeMatch(SDR1, SDR2, size, matchThreshold):\n",
    "    match = {}\n",
    "    match['overlap'] = len(computeUnionAndOverlap(SDR1.activeBits, SDR2.activeBits)['overlap'])\n",
    "    match['overlapAsPercentageOfSDRSize'] = (match['overlap'] / size) * 100\n",
    "    match['isMatch'] = matchThreshold < (match['overlapAsPercentageOfSDRSize'])\n",
    "    \n",
    "    return(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create some SDRs and see if we consider if they match based on a matching criteria we will provide of $10\\%$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 2048\n",
    "activeBits = 100\n",
    "matchingThreshold = 0.1\n",
    "\n",
    "s1 = SDR(size, activeBits, \"SDR1\")\n",
    "s2 = SDR(size, activeBits, \"SDR2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computeMatch(s1, s2, size, matchingThreshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for s1 and s2, they have 6 bits that overlap, or .29%. This is above our matching threshold of .1% \n",
    "\n",
    "<b>Noise tolerance of SDRs</b><br/>\n",
    "\n",
    "Finally, let's consider one more question: if we then randomly change some bits in each of the SDRs (meaning we randomly turn some zeros to ones, and randomonly turn some ones to zeros), how will our match threshold be affected. If we could flip a substantial number of the bits and still meet our match threshold, this would suggest that the ability to create matches between SDRs is quite tolerant to noise.\n",
    "\n",
    "To explore this, first let's create a function to randomly flip bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomly_flip_percentage_of_bits(sdr, sdr_size, percentage_to_flip):\n",
    "    rd.shuffle(sdr)\n",
    "    count_of_bits_to_be_flipped = int(percentage_to_flip * len(sdr))\n",
    "    new_sdr = sdr[count_of_bits_to_be_flipped:]  \n",
    "    noise = create_randomised_sdr(sdr_size, count_of_bits_to_be_flipped)\n",
    "    sdr_with_noise = noise + new_sdr\n",
    "    return(sdr_with_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 2 SDRs already set up (s1 and s2), and they are match based on matching criteeria of []. Let's use our new function to randomly flip $40\\%$ of bits in s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdr_size = 2048\n",
    "active_bits = 41\n",
    "matching_threshold = .1\n",
    "\n",
    "s1 = create_randomised_sdr(sdr_size, active_bits)\n",
    "s2 = randomly_flip_percentage_of_bits(s1.copy(), sdr_size, .33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see if the y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_match(s1, s2, sdr_size, matching_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specs \n",
    "n = 2048\n",
    "w = 41\n",
    "spars = 0.02\n",
    "\n",
    "1 - generate the specs\n",
    "2 - generate the same SDR but flip 33% of the entries to noise\n",
    "3 - Look at the resulting overlap score \n",
    "\n",
    "So 33% of noise gives overlap of 28\n",
    "\n",
    "Leads to fuzzy matching - use a value $\\theta$ - threshold of overlap score\n",
    "\n",
    "We can dial theta up and down \n",
    "\n",
    "INteresting - you can set theta, then change noise - idea of how much noise we can add and still get a match....\n",
    "\n",
    "interesting...we can consider a formula of if something is a false postive - math means astronomical small percentage of a match.....SDRS\n",
    "\n",
    "Even when we dial up to 50%, we get hardly any change of a false positive\n",
    "\n",
    "SDRs have massive resistance to noise\n",
    "\n",
    "an then do noise for 32 x 64\n",
    "\n",
    "A quick asidd on these visualisations - we are really looking at an array, but the visualisation its a matrix. This feels like a bit of gray area, periodic, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#99ddff; color:black; padding: 10px\">\n",
    "<b>Add to these notes:</b>\n",
    "    \n",
    "What becomes apparent after doing is that it needs more rigor around the code. Ideally I want to pass in size SDR get specs, get overlap and union, set a threshold. It makes more sense to have and SDR class in python which can then have attributes\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have covered alot in this notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
